{
  
    
        "post0": {
            "title": "Build (and Run!) Your Own Image Classifier using Colab, Binder, Github, and Google Drive (Part 2)",
            "content": ". Building your own image classifier is fun! . This is Part 2 of a two-part article on building your own image classifier. . If you haven’t read the Part 1 yet, please read it first as this second part continues where the Part 1 left off. . Here’s the link to Part 1. . Sharing your app to the world . In order to share it to the world, we will be using Binder , another cloud based service that can run Jupyter notebooks as applications (much like Colab can also run Jupyter notebooks). . The difference is that Colab is an interactive editor environment (allowing you to edit and run your Jupyter notebooks) while Binder is just for running your Jupyter notebooks as applications. . Create a Github Repo . In order to run your Jupyter notebook on Binder, we will need to create a Github repository (also known as a repo), which is a place to store the associated files necessary to run your app. . This includes the exported image classifier named export.pkl file which we built earlier. . If you haven’t done so earlier, you’ll need to create a Github ID, which grants you access to the Github site and allows you to create your own repos. . Click on the link here to create a Github ID. . Once you’ve created a Github ID, you can now create repos on Github. . Click on this link here to generate your repo. . (Make sure to change the name of project to something that is NOT the same name as original name build-your-own-image-classifier-template). . This is to work around a limitation of Github regarding public forks. Naming your project build-your-own-image-classifier (without the -template) is fine, but naming your project to something unique for yourself would be better. . Please note down the name of your Github ID, your Github password as well as the name of your repository (or repo for short) as you will use them in the next steps below. . Upload your exported image classifier file to Github . Once you’ve created your Github repo, you are now ready to “push” or copy the exported image classifier from your Google Drive to your Github repo. . (If you’re wondering why can’t we just download the exported image classifier from Colab and upload it to your Github repo – it’s because of a limitation of Github which restricts uploads of files bigger than 25MB. Unfortunately, your exported image classifier file (export.pkl) is usually bigger than this so we have to use git - a version control system used by Github, to move the exported image classifier from Colab to our Github repo) . Click on the button below to run another notebook on Colab which will do this. . Fill out your github repo, github id and password as explained in the notebook. . . Once you have done this, your exported image classifier should now be visible in the list of files of your Github repo. . Your Github repo URL should have the format https://github.com/&lt;your-github-id&gt;/&lt;your-github-repo-name&gt;. . . Please check that the export.pkl file is in the list of files of your repo. . (If it’s not there, then something went awry with the previous step – maybe your github id, github repo or password were entered incorrectly. You might need to rerun the previous step above.) . Run your app on Binder . In order to run Binder, please take note of url of the Github repo you created in the previous step. . Next, head over to Binder by clicking here. . Configure Binder as shown below. . . In the GitHub repository name or URL field (marked with the text 1), paste the url of your Github repo. . In the Path to a notebook file (optional) field (marked with the text 2), enter /voila/render/build-your-own-image-classifier.ipynb. . Change the path type (marked with the text 4 right next to the Path to a notebook file (optional) field) from File to URL.(This will change the label of the previous field to URL to open (optional)) . Make sure to fill out the three fields correctly. . Once done, click on Orange-Yellow “Launch” button (located right next to the URL to open (optional) field) and you are all set. . (Note that starting your application the first time will be slow, as Binder must first assemble your application into a ready-to-run format. The next time your application starts, it will start up a little bit quicker.) . Note down the Share URL to your app (marked with text 3 in the above image) so you can share it with your friends and family to explore. . Copy and Share your Binder App Link . When you run your app on binder, Binder provides a link to rerun your app. Save the link and share that link to your friends and family so they can run your application. . Profit!! . If you followed the steps above, you now should have image classifier running on Binder so its time for Profit!! . What we learned . Learning to do Deep Learning (of which building Image Classifiers is part of) is definitely within the reach of ordinary folks like us. . | You don’t need to have deep pockets to do Deep Learning. . | You don’t always need to have lots of data to do Deep Learning. . | . Want to learn more? . If you want to learn more (we’ve just scratched the surface), take this totally free course (no Ads!) called Practical Deep Learning for Coders – it will teach you Deep Learning from top to bottom, using running code, on Jupyter notebooks like the ones we used on Colab, and build useful applications beyond the simple image classifier we built here. . You can also join an inclusive global learning community that welcomes beginners and experts alike and ready to help you start your deep learning journey. . Hope to see you there! . Final Note and Acknowledgements . The software used to clean your data, build your classifier and run your application is largely based on the fastai python package, written by Jeremy Howard and Sylvain Gugger. . The Jupyter Notebook environment that enables us to run interactive documents to build our image classifier won the 2017 ACM Software System Award and is widely used by scientists, researchers and students around the world. . The software package used to collect the images (jmd_imagescraper) was built by Joe Dockrill (@joedockrill), one of the students of the fast.ai course. . The fast.ai course Practical Deep Learning for Coders is considered one of the best introductory courses on Deep Learning, which is taught by Jeremy Howard and Rachel Thomas, who are also the founders of fast.ai, a non-profit organization working towards democratizing the use of AI in the world. . There’s also a book “Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD” available on Amazon, but the authors (Sylvain and Jeremy) have made the whole book available as Jupyter notebooks for free as well. . The steps to run on Binder were based on a forum post created by Vikrant Behal (@vikbehal), another student of the fast.ai course. . If you liked this article, and most specially if you were able to build your own classifier based on this article, give me a shoutout on my twitter account @butchland or message me on the fastai forums: @butchland, I’d really appreciate it! . .",
            "url": "https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/10/05/byoic-on-colab-part2.html",
            "relUrl": "/machine%20learning/2020/10/05/byoic-on-colab-part2.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Build (and Run!) Your Own Image Classifier using Colab, Binder, Github, and Google Drive",
            "content": ". Building your own image classifier is fun! . This is Part 1 of a two-part article on building your own image classifier. Here’s the link to Part 2. . So here’s what were building – A pet classifier! . First, head over to Binder to see it in action – click on the button below (where it says “launch binder”). . Play it with for a while (make sure you upload some photos). Check out how well (or how bad) it classifies your pets and come back to this page to continue. . . Warning: it takes a while for it to start, so please be patient, I promise it will be worth the wait! . What’s an image classifier? . I hope you clicked on the “launch binder” button above and played with the pet classifier app before coming back here to continue reading the article. . If you did play around with app above (like I told you to), you’d know that an image classifier classifies images. . This article will show you how to build your own image classifier – by default, you’ll be building a pet classifier just like in the demo above, but you can tweak it so you can build an image classifier for whatever you want (as long as you can find enough pictures of it to serve as examples). . Some prerequisites . Because we’re using cloud services to build everything in this project (no need to install anything on your computer), you’ll have to signup for some services, so you’ll need to have these. . A Gmail ID (almost everyone has one these days, don’t they? But if you don’t, head over to this link here right now and create one.) . | A Github ID (don’t worry if you don’t have one, we can create it later – but if you’re in a hurry, you can create one right now – just click on this link here) . | . Steps to building your own image classifier . So now we’re ready to start building! . Here are the steps: . Collect some pictures – if you don’t have pictures, you can usually search for them over the internet. . We will be using Colab, a Google cloud service used by data scientists and machine learning researchers and students around the world to build and study machine learning models. . | Once you’ve collected your pictures, you’ll need to clean your data – you’ll see that the images you get through an internet search are sometimes incorrect, so you need to clean up your data first if you want your image classifier to work well – after all, like in a lot of things, if your image classifier gets Garbage in, it puts Garbage out) . | Now that you have cleaned your data, you can build a Neural Network Image Classifier that can classify images. . (Neural Networks a.k.a. Deep Learning Artificial Neural Networks are currently the best way to build image classifiers – that’s why we are using them!) . | Once you have your image classifier, you need to export it to a format so that it can be used just like a regular program – you feed it an image, it will classify that image. . | We will also build a very simple app that can use that exported image classifier. . We can test it out on Colab, but in order to make it available for others to use, we’ll need to use a cloud platform that can run your app. . In our project, we will be using Binder, a free cloud service that can run your image classifier. . | Next, you’ll need to move that exported image classifier to Github (along with your app) because that’s where Binder will get it from in order to run it. . | Once you get your exported image classifier on Github, you can now run your image classifier app on Binder. . | The next step is … Profit! – as the whole world starts using your app… (or maybe just your friends :) . | . Don’t worry if you haven’t figured out what to do next, I’m just outlining the steps of the whole process, but I’ll be walking you through the process step by step. . I’ll even try to explain what we’re doing as we’re doing it, but even if you just follow the instructions (without understanding) you should just do fine (but understanding what you’re doing makes it more fun, doesn’t it?) . Collecting Images . In order to build a Deep Learning Image Classifier, we need data. . This data (in the form of labeled pictures) will be used as examples from which the Neural Network learns to distinguish between different categories. . In our case, our Neural Network Image Classifier distinguishes cats from dogs. . So we will need pictures of cats and dogs. Not only do we need pictures of cats and dogs, we will need to label each picture as containing either a cat or a dog. . Introducing Colab . As previously stated, we will be using Colab in order to do this. . So you might be wondering, what is Colab and how do I use it? . Colab is an interactive document editor, much like Google Docs, but unlike Google docs, you can run code inside it. . These Colab documents are called Jupyter notebooks, and they are stored in your Google Drive, just like Google Docs, Sheets or Slides. . Jupyter notebooks contain both prose (such as text, graphs, and images) as well as code. . You can also download them to your local drive (with the file extension .ipynb) or store them on Github, which is a cloud-based public repository for code and documents. . I’ve built Jupyter notebooks for each step of building your own image classifier, so we can just run them one after another until we’ve built our image classifier. . The nice thing is, because Jupyter notebooks contain both text and code, I’ve put in explanations about what the code is doing alongside the actual code (kind of like code comments, but better formatted.) . Running your first notebook to collect images . Click on the button (where it says “Open in Colab” ) below to open a copy of the notebook. . If you haven’t already logged in to Google, you’ll have to login first. . Follow the instructions in the notebook. . . At the end of the steps in that notebook, you should have a labeled image dataset for your image classifier stored on your Google Drive. . (If you didn’t modify the default project name, the file pets.tgz should be in your Google Drive under the folder /My Drive/build-your-own-image-classifier/data/pets) . IMPORTANT: Make sure to terminate your Colab session once you’ve completed this task (in order to avoid using up unneccesary resources). . Cleaning your Image Dataset . Your next step is to clean that dataset. . I’ve created another notebook do this (this makes it easy to keep the notebook short and easy to understand). . So click again on the notebook below. . . After running the notebook above, you should now have a cleaned up image dataset stored on your Google Drive. . (If you didn’t modify the default project name, the file cleaned_pets.tgz should be in your Google Drive under the folder /My Drive/build-your-own-image-classifier/data/pets) . Building a Neural Network Image Classifier . You’re now ready to build a Neural Network Image Classifier. . This step is also known as training a neural network model. . In addition to training the model, we also need to export it to what is known as a “pickle” format (into the file named export.pkl). . Again, click on the button below and follow the steps outlined in the notebook. . . At the end of running this notebook, you should have an exported classifier(export.pkl) in your Google drive. . (If you didn’t modify the default project name, the file export.pkl should be in your Google Drive under the folder /My Drive/build-your-own-image-classifier/models/pets) . Test your Image Classifier App on Colab . We can now test our image classifier app on Colab. . Again, click on the button below to test the application on Colab. . . Once your image classifier app has been tested, you have accomplished your goal – to build an app that runs an image classifier. . But wouldn’t it be great if you could make it easy for others to use your app? . After all, you are now ready to share it to the world (or at the very least, your friends, family and colleagues…) . (TO BE CONTINUED IN PART 2) . If you liked this article, and most specially if you were able to build your own image classifier based on this article, give me a shoutout on my twitter account @butchland or message me on the fastai forums: @butchland, I’d really appreciate it! . Acknowledgements . The software used to clean your data, build your classifier and run your application is largely based on the fastai python package, written by Jeremy Howard and Sylvain Gugger. . The Jupyter Notebook environment that enables us to run interactive documents to build our image classifier won the 2017 ACM Software System Award and is widely used by scientists, researchers and students around the world. . The software package used to collect the images (jmd_imagescraper) was built by Joe Dockrill (@joedockrill), one of the students of the fast.ai course. . .",
            "url": "https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/09/21/byoic-on-colab.html",
            "relUrl": "/machine%20learning/2020/09/21/byoic-on-colab.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fast.ai Deep Learning Adventure Treks (Part 2)",
            "content": ". fast.ai Deep Learning Adventure Treks (Part 2) . Summary . fast.ai Deep Learning Adventure Treks is an initiative to provide first-time fast.ai learners with a more formalized, structured study group, with a fixed schedule, led by more experienced fast.ai community members on a volunteer basis, to make the online course (MOOC version) more engaging and effective specially for non-programmers. . Part 1 covers the rationale. Part 2 covers the program’s vision and use cases. . Vision . In order to give a better picture of how the program can be implemented, we created user personas to provide a more personal, relatable picture of the program’s use cases. . User Personas . Hue is a dermatologist from Vietnam who wants to develop telemedicine diagnostic mobile apps. | Nadja is an agriculturist who is working with rice farmers from Bangladesh. She is hoping to combine satellite and drone images with agricultural field reports and weather forecasts to improve crop yield estimates. | Sonia is a health informatics expert who is hoping to use data from her country’s health insurance filings improve public health policy. | Nikolai is a journalist studying his country’s social media growth patterns and how it has influenced governmental policies. | . They are all hoping to add deep learning to their toolkits and want to learn how to apply it to their domains. . Pre-implementation Scenarios . Hue’s setup issues . Hue has just learned about the fast.ai MOOC from her twitter feed and is interested in taking the course. She doesn’t have a programming background and is a little intimidated by the topic. She is however very determined and is hopeful after watching the first lecture that she can apply deep learning to help her in diagnosing skin diseases. | She plans to work with some mobile devs who may not have the deep learning skillsets, but she plans to develop the diagnostic deep learning models herself (since she knows how to diagnose dermatological problems from images) and serve the models via an API. | After having some problems setting up her jupyter notebook environment, she joined the forums and asked for help. While there were some responses to her questions, it took her several days to resolve them, primarily due to timezone differences. | She also felt a little lost with some of the advice, as in most of them assumed a deeper computer technical background than what she had. | She felt she could have resolved her issues a lot quicker, had their been some hands-on assistance. | She is also looking for assistance in learning Python so she can better handle the course material. | . Nadja’s customized workflow applications . Nadja also heard about the fast.ai MOOC from a colleague. Although her work already involves analyzing satellite images and weather forecasts separately, she’s interested in a customized workflow that integrates all those data sources into deep learning models so they can scale up personalized predictions and insights for the farmer’s crop yields. | Although her office employs data scientists, none of them have a lot of experience with deep learning and she’s reluctant to bother them for assistance. | She uses the forums as much as she can, but wishes for a more person-to-person interaction even if only an online one, since her goal of combining images with weather forecasts is not a common thing being done in her field. | . Sonia’s schedule . Sonia is hoping to add deep learning to her data science skills. She has been using Python in her other projects, and is interested in building a recommendation system for local goverment units based on public health data from health insurance claims of their constituents to hopefully improve public health outcomes. | She started watching the videos and participating in the forums. She is eager to learn new skills, but also realizes her intended project is quite ambitious and instead is working on smaller, simpler projects. | Her problem is sustaining her interest and momentum. While she welcomes the fact that the MOOC allows her to learn on her own pace, she also knows that a regular schedule with defined weekly objectives can help her maintain a consistent pace to her advancement in deep learning. | . Nikolai’s networking goals . Nikolai is a data journalist and is familiar with Tableau and a smattering of R, but not python. He intends to learn deep learning especially NLP techniques, in order to apply them to preprocessing text from social media sources to be used for analysis later. | Nikolai has been watching the videos and participating in the forums, but is also hoping for more social interaction with the community members. | While he’s trying to gain as much understanding of deep learning as he can, he is also hoping he can build connections from his country’s local fast.ai community and leverage their diverse talents to focus on problems he’s interested in. | . A vision for the fast.ai Deep Learning Adventure Treks . The goal of the fast.ai Deep Learning Adventure Treks program is to help learners of the fast.ai MOOC become more effective. A following idealized scenario gives us a glimpse of how the initiative can address the issues discussed above. . A Deep learning expedition . Hue finds out about the fast.ai Deep Learning Adventure Treks. She looks for and finds a program starting in about a week with a schedule geared for her timezone. | She signs up and attends an online orientation via a Zoom call. This particular Adventure Trek program is run by Zach, a long time fast.ai volunteer. She’s also happy to find that Nadja, a fellow school alumni, has also signed up for the same group. | As part of the program requirements, they sign a fast.ai Deep Learning Adventure Trekker’s agreement that asks for their commitment to the community’s values of inclusion and respect as well as to exert their best efforts to completing the program. They also commit to helping out other participants and contribute to the community in the future. | The orientation also included introductions as well as some online social games to break the ice among the group members. | She and her fellow expedition members (all 7 of them) are given a proposed schedule by Zach, their guide. They discuss it and modify it based on their personal schedules (A sample schedule is provided below). | Based on her schedule, she requested if she could skip the group video watching activities, but promised to join the discussion after (she planned to watch the videos on her daily commute instead) | As part of her mini-projects, she’s signed up to pair with Nikolai, who seems interested in NLP, to recreate a twitter sentiment analysis notebook. While she doesn’t plan on using NLP in her project, she’s also keen to round out her knowledge of deep learning to include areas other than where she plans to specialize in. | She’s also thinking of teaming up with her fellow group member Sonia as well as her friend Nadja for their final group project, since Sonia has some programming skills, which she and Nadja did not. Besides, they seemed to have bonded quite well during the orientation. | She’s also thinking of turning her proposed team into an accountability group, a recommendation from their Adventure Trek guide. By making herself accountable to an external group, she hopes to become more consistent in completing the study group activities and make her deep learning study habits stick. | . Sample Program Schedule . Base Camp (Orientation) | Station 1 (Intro) video watch Lecture 1 + Q&amp;A discussion after | study group discussion workshop activity: setup jupyter notebook environment | . | book reading activity (Chapter 1) + Questionnaire roundtable discussion | run notebook 1 + troubleshooting session | . | Station 2 (Production) video watch Lecture 2 + Q&amp;A discussion after | study group discussion workshop activity: setup Azure search keys + run binder | . | book reading activity (Chapter 2) + Questionnaire roundtable discussion | run notebook 2 + troubleshooting session | . | Station 3 (MNIST) video watch Lecture 3 + Q&amp;A discussion after | study group discussion | book reading activity (Chapter 4) + Questionnaire roundtable discussion | run notebook 4 + troubleshooting session workshop activity: build a nn model from scratch | . | . | Station 4 (Pets) video watch Lecture 4 + Q&amp;A discussion after | study group discussion workshop activity: exercises using Datablocks | . | book reading activity (Chapter 5) + Questionnaire roundtable discussion | run notebook 5 + troubleshooting session | . | Station 5 (Ethics) video watch Lecture 5 + Q&amp;A discussion after | study group discussion | book reading activity (Chapter 3) + Questionnaire roundtable discussion | . | Station 6 (Multicat) video watch Lecture 6 + Q&amp;A discussion after | study group discussion workshop activity: redo pets and mnist using multicat | . | book reading activity (Chapter 6) + Questionnaire roundtable discussion | run notebook 6 + troubleshooting session | . | Station 7 (Collab and Tabular) video watch Lecture 7 + Q&amp;A discussion after | study group discussion | book reading activity (Chapter 8) + Questionnaire roundtable discussion | run notebook 8 + troubleshooting session | book reading activity (Chapter 9) + Questionnaire roundtable discussion . | run notebook 9 + troubleshooting session | . | Station 8 (NLP) video watch Lecture 8 + Q&amp;A discussion after | study group discussion | book reading activity (Chapter 10) + Questionnaire roundtable discussion | run notebook 10 + troubleshooting session | book reading activity (Chapter 12) + Questionnaire roundtable discussion | run notebook 12 + troubleshooting session | . | Summit (Group Project) group presentations | . | . Sample Schedules . video lecture watching - Tuesdays 6-8pm every 2 or 3 weeks | video lecture Q&amp;A discussion - 8-9pm Tuesdays following video watching | video lecture study group Q&amp;A - Thursdays 6-8pm every 2 weeks | book reading chapter - Tuesdays 6-9pm after video lecture watching | book chapter questionnaire Q&amp;A - following book reading | run notebooks and troubleshooting - Thursdays 6-8 after book reading | group projects - presentations scheduled 2 weeks after last book reading. | . Additional Activities . In addition to programmed study groups centered on the fast.ai lectures, additional related topics may also be covered (depending on the students’ learning needs), such as: . Python programming | git | pandas | scikit-learn | . Moreover, participants can also undertake individual mini-projects. The Adventure Trek guides also curate and provide pointers to new members on what mini-projects they can undertake to level up their deep learning skills given their current skill level. . As envisioned, these activities can help fast.ai MOOC course increase the number of learners who go on to finish the course and more effective in achieving its goals. . Motivations for the fast.ai Deep Learning Adventure Trek Guides . So aside from altruistic motives, why would an fast.ai Deep Learning Adventure Trek Guide volunteer? . The primary reason, I believe, is because of the truism that the best way to learn a topic is by teaching it. It will help the guide gain a deeper understanding of deep learning, as well as cementing the foundations into their knowledge base. . Another reason can be that providing social interactions in a supportive environment is a reward in itself. . Lastly, the ability to teach Deep Learning is a valuable skill in of itself, and may lead to opportunities later. . Leading an adventure trek provides valuable experience to develop these skills, as well as exposing the guide to a diverse set of domains where deep learning can be applicable. . Further incentives (such as certification, reviews, or financial remuneration) maybe added in the future, based on feedback. . .",
            "url": "https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/07/06/fastai-deeplearning-adventure-guides-part2.html",
            "relUrl": "/fastai/2020/07/06/fastai-deeplearning-adventure-guides-part2.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "fast.ai Deep Learning Adventure Treks (Part 1)",
            "content": ". fast.ai Deep Learning Adventure Treks (Part 1 - Rationale) . Summary . fast.ai Deep Learning Adventure Treks is an initiative to provide first-time fast.ai learners with a more formalized, structured study group, with a fixed schedule, led by more experienced fast.ai community members on a volunteer basis, to make the online course (MOOC version) more engaging and effective specially for non-programmers. . Rationale . My fast.ai experience . When I first joined the fast.ai MOOC (Practical Deep Learning for Coders) around January last year, I first felt very enthusiastic and energized to view the lectures, run the notebooks, and participate in the forums – as a result, I’ve learned a lot from the first 4 lectures. . But as the weeks passed, my interest waned, not because I wasn’t learning anything – quite the opposite – I learned more from the fast.ai MOOC about applying Deep Learning in the real world than any other ML course I had previously taken. . I still listened to the last 3 lectures, I but didn’t do the additional coursework that would have cemented the concepts into my mind. . By the time the 2nd part of the course came, I had an even harder time catching up, and thus ended my interest in the latter parts of the Part 2 of the course. . A major factor for this waning interest is that learning remotely can be quite isolating – and despite having a fantastic and supportive community in the forums, I felt at a loss in figuring out what personal projects I could pursue to further deepen my understanding of the topics. . Though I tried to maintain a weekly schedule (following the live course’s pace), there was nothing that really pushed me to complete something each week. . I suspect that I am not alone in this – there may have been a lot of MOOC learners whose interest would not have waned had there been more support for their learning needs. I know a learner’s individual tenacity is key to success in this course, but anything we can do to make it more effective is worth pursuing, I think. . So when I got invited to the 2020 live online edition of the fastai course, I was stoked and energized again to deepen my understanding of machine learning and deep learning. . This time around, I resolved to make my learning more consistent and effective. . I think I did a lot better this time around – I was able to finish the 8 lectures and I was more active in the forums and did the course work assignments more religiously than last time. . Moreover, as of today, almost two months after the live online course officially ended, I have kept up my interest and am still doing everything I can everyday to deepen my understanding of the field (Of course, another factor that may have inadvertently helped was the quarantine due to COVID-19). . However, I believe the biggest factor in my sustained interest this time around was due to the fact that I joined an online study group during the live online weekly lectures, – actually 2 study groups: the Unofficial SF Study Group (despite the fact that I was based in a timezone 16 hours ahead), and the beginners study group led by Wayde Gilliam (@wgpubs) every Thursday. . More importantly, the Unofficial Study Group resolved to continue meeting even after the end of the live lectures last May 6 to further encourage its members to keep pursuing their learning. . It has since evolved into 3 related activities: . A biweekly meeting for presenting projects such as kaggle competitions, and blog posts or software projects | A weekly book rereading where participants set aside a 3 hour block of time to read a fastbook chapter followed by 30 minute discussion | An accountability mini-group where we meet weekly to discuss our weekly learning goals and encourage accountability for those goals. | . Lessons learned . For me, these activities have not only been effective in keeping my interest in the course (aside from participating in the forum discussions) but have also helped relieve the isolation brought on by the COVID-19 pandemic quarantine. . Aside from these, the fast.ai community has (especially during the live lectures) also provided additional activities – I would highlight the videos and meetings on the fastai source code review as well as other videos by Zach Mueller. . These resources have all helped enrich my learning experience, but I think the social experience given by an online study group was the most effective addition to make my fast.ai learning experience more effective. . Suggestions for Improvement . Having had a long experience with conducting trainings on software development, I have always thought about how to make the online fast.ai learning experience more effective not only for myself but for others like me. . I totally support fast.ai’s goal of democratizing AI and Deep Learning and making it more accessible. . An important part of this effort is making it more accessible outside of the US, in less developed countries like Asia and Africa (which is why making the lecture transcripts and captions available in other languages is also important). . Also important is making the community more diverse and inclusive, especially in encouraging people from different backgrounds to join the community, making them feel welcome and helping them achieve success in learning deep learning so they can apply it to their domains and societies. . I think one area where the fast.ai support is especially lacking is helping those learners who may not have a programming background. . While the fast.ai’s “whole game” approach and goals encourages people who may not have a strong programming background to learn deep learning and apply it to their own fields, I think we can provide better ways to support them and get over the initial hump of learning the Python basics (plus other software development related stuff like git, using the bash command line, etc.). . I think this is quite possible for the fast.ai community to achieve – given that the majority of its members, I believe, do have programming chops to teach the other learners who might be having difficulty in that area. . For these reasons, and based on my experiences, I have come to propose the fast.ai Deep Learning Adventure Treks program as an initiative to help make the online fast.ai learning experience more effective. . continued on to Part 2 which covers the program’s vision and use cases. .",
            "url": "https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/28/fastai-deeplearning-adventure-guides.html",
            "relUrl": "/fastai/2020/06/28/fastai-deeplearning-adventure-guides.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
        ,"post11": {
            "title": "Accountabilities",
            "content": ". Accountabilities . One of the problems with online learning in general is that it can be a pretty lonely pursuit. After a while, if there is no one to push you, motivation can flag and your progress slows down. . In a bid to prevent this, I have been active in a study group so that I have continuing activities even as we await the second part of the 2020 edition of the fastai course. . Some of the activities we have is a biweekly meeting focused on presentation of projects, blog posts, etc. as well as a weekly meeting focused on reviewing the lectures and the book chapters. A third activity is a weekly meeting with a smaller group (in my case, just me, Ope and Maryam) to become accountability buddies. . As part of making my accountabilities stick, I am publishing it publicly (just not searchable) so I can share my plan with my friends or study group mates and make myself accountable to them. . Update 6/26/2020: I haven’t been able to meet my deadlines lately and so I’m adding this to reflect my updated priorities and strategies – in order to reduce my cognitive burden in maintaining schedules using this post to keep track of my targets, I’ve decided to just limit my deadline to the next thing I need to finish, and will just keep updating my deadlines for the next tasks on completion of the pending tasks. I’ll try to keep my accountability worksheet updated however, as its much easier to update that. . My Goals . My goal is to acquire both the theoretical foundations of Machine and Deep Learning as well the practical skills to apply deep learning to particular fields such as computer vision, natural language processing and tabular data, as these are probably the most accessible applications for me right now. . In addition, I also want to learn to develop and deploy ML applications on cloud platforms such as GCP, AWS and Azure. . My Accountabilities . My focus right now (as of June, 2020) is to deepen my understanding of deep learning. I plan to focus on the fastai materials for now. My goal is to build a set of lecture notes combining the video lectures and the book chapters. . In addition to gaining a theoretical understanding of the topics, I also plan on developing my skills in building and training models. My goal is to build around five mini-projects with either pre-existing datasets or datasets that I build. . I also plan on joining a kaggle competition in about a couple of weeks time (once I have started on the lecture notes and mini-projects). . SMART Goals 1 . Lecture Notes - review video, read book, answer chapter questionnaire, review other peoples answers, run notebooks, rebuild notebooks from scratch, explore notebook variants, and finally write and publish lecture notes (the deadline of each lecture note is before the start of study group reading meeting for the next lecture). See tracker . Lecture 1 (app_jupyter and ch1 intro) and Lecture 2 (ch2 production) - June 2, 2020 (edit 6/26: finished ch1 intro and reset sched for ch2 to June 29,2020) | Lecture 3 (ch 2 production and ch4 mnist basics ) and Lecture 4 (ch4 mnist basics and ch5 pet breeds) - June 9, 2020 (edit 6/10: reset sched to TBD - after completion of ch2) | Lecture 5 (ch 3 ethics) and Lecture 6 (ch5 pet breeds and ch6 multicat) - June 16, 2020 (edit 6/26: reset sched to TBD after completion of ch5) | Lecture 7 (ch 8 collab and ch 9 tabular) - June 23, 2020 (edit 6/226: reset sched TBD after completion of ch6) | Lecture 8 (ch 10 nlp and ch 12 nlp dive) - June 30, 2020 (edit 6/26: reset sched TBD after completion of ch 9) | . | Mini projects (todo) - should cover computer vision, nlp, tabular, collab filter . Baybayin (filipino script) handwriting recognition system initial data download and exploration (eda) | initial model build | TODO: build canvas image writer web app with baybayin recognition (with pytorch js) ? github: Hand-Written-Digit-Recognition | github: nepali-digit-recognizer | . | . | Selfie/Faces - gender recognition system ? | Twitter sentiment140 analysis (NLP) ? | Image Recognition (face) celebrity ? | fingers recognition collect fingers data | explore kaggle | explore koryakin fingers | test koryakin fingers model | . | SVHN house numbers recognition | . | Long term project (todo) - build up ML Portfolio . Kaggle competition ? | Detecting OOB (out of bounds) data | Filipino caption translation of fastai videos ? | Deep Learning Adventure Guide ? | . | Fun projects (todo) . DDG Image Downloader enhancements to use full size orig images current project (using bing images) | nbdev generated page for project | . | Update my pet-breed-classifier-demo to use fastai2 | React Native mobile app with image recognition (template) | React/Vuejs Webapp with image recognition (template) ? | Godot game with image recognition or NLP (template) ? | . | . Footnotes . SMART Goals - Specific, Measurable, Achievable, Relevant, Timebound &#8617; . |",
            "url": "https://butchland.github.io/butchland-machine-learning-notes/life%20hacks/2020/05/30/accountabilities.html",
            "relUrl": "/life%20hacks/2020/05/30/accountabilities.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Machine Learning Foundations",
            "content": ". Introduction to Machine Learning, Deep Learning and Neural Networks . These notes are my attempt to implement Feynman’s method of learning, whereby, from memory (with as few glances to Wikipedia as I can), I attempt to reconstruct my understanding of deep learning so far.1 . Since I am trying to recreate this from memory, I may not have references to some of the stuff I’m saying – but every now and then, as I update this (and possibly other) document, I might include them just to make it easy to confirm what I’m saying. . Last note: this document will be rambling jumble of thoughts that hopefully over time will be reorganized better into a wiki. There will be lots of repetition of ideas and maybe some of it will be wrong, but as I go over them, hopefully this will occur less and less. . Last last note: I am writing this from the perspective of me trying to explain to a fellow developer how machine learning works. Its not very in depth yet, but over time I hope to go deeper into the topics all the way to showing the code while explaining the theory. . Outline and Topics . Definitions - I still plan to organize this – this is just a plan for what topics to cover.) . Machine Learning | Neural Network Model | Model Training | Deep Learning | Gradient | Gradient Descent | Stochastic Gradient Descent | Loss and Loss Function | Activation Function | Linear Function | Optimization | Learning Rate | Training, Validation and Test Datasets | Parameter Initialization and Transfer Learning | . | . Foundations . Machine Learning . Machine Learning (ML) is an alternative way of programming a computer whereby a computer learns to perform some task by being given some examples. . This is in contrast to the usual way of programming computers to do a task by specifying the steps in some detail. . The mechanism by which ML accomplishes this process of learning to do tasks through examples is by using neural network models. . A Neural Network Model is a program that can learn to perform tasks by being shown examples. . Using a set of given examples, a model: . tries to perform a task, and | its performance (i.e. how well it performs that task) is then measured, and | that measurement is then used to adjust the model in such a way that affects (hopefully improves) its performance. | . As a concrete example, if we wanted to create a cat-dog image recognition program, we could start with a model and a lot of cat and dog images. . The model first tries to predict that given the images as input, it makes guesses as to whether each example is a cat or dog. . | Its performance (whether it guessed correctly or not) is then measured in some way, . | And that measurement is then used to adjust the model so that the next time it tries to make a prediction, it does a better task. . | And the task is repeated again and again, with the model (hopefully) continually improving until its good enough at distinguishing cats from dogs. . | . Model Training . This process of performing a task, measuring its performance, and adjusting the model is known as training the model or model training. . Once a model’s performance reaches an optimum level (e.g. a cat-dog image recognition neural network model correctly distinguishes cats from dogs 99.995% of the time), it can then be used to perform that task just like any other computer program. . The Model as a Function . A good way to approach machine learning is to look at a Neural Network Model as a function that computes an output given a set of inputs. . Neural Network Models and Neurons . A model is basically a function that is composed of a set of processing units interconnected in a particular architecture. Each processing unit is called a neuron or a node, and each neuron takes in a set of inputs (which are numerical input values), and outputs a result (the numerical output value). So we can also consider these neurons as functions as well. . Parameters, Weights and Biases . Associated with each neuron is a set of parameters. This set of parameters can be further divided into weights and biases. Associated with each input to a neuron is a weight, which is a type of a parameter. Associated with each neuron itself is a bias, which is the other type of parameter. The output of each neuron can then be the inputs to another set of neurons, which combine inputs and outputs to inputs and outputs of other sets of neurons and to finally to output a result (which may be a category, a number or a set of numbers) which is the output of the model itself. . Neural Network Layers . These neurons are usually organized into layers, with the outputs of one layer of neurons feeding into the inputs of the next layer of neurons. . Activation Functions . Between the connections of these neurons are the activation functions, which transform the output of the previous layer in some way before feeding it as the input of the next layer. . Architecture . The way these layers interconnect their neurons is known as the architecture of the model. . Deep Learning . Deep learning is the term associated with the usage of neural networks that have many layers and is the primary reason for the revival of machine learning as a useful tool in many fields today. . An Image Recognition Example . As an example, we can say that a trained cat-dog image recognition neural network model is a function that can take as an input an image and outputs a result, telling us whether the image is a picture of a cat or a dog. . The way we can make the model do this is by adjusting the parameters of the model in such a way that it can do the task of image recognition – recognizing cats from dogs. . So now the problem becomes how do we come up with a set of parameters for the model so that it does the task well. This is done by training the model with example images of cats and dogs. . Training the Model . In order to train a model, it is given a set of inputs, known as the training dataset (which for the image recognition task of distinguishing cats from dogs, the inputs are images of cats and dogs). . Aside from the input data, there needs to be some target output (also called a label) associated with each input, which in the case of cat-dog image recognition task, is a label telling us that each input image is either a cat or a dog. So we can’t just have pictures of cats and dogs, we need to have labelled examples of cat and dog images. . The task of an image recognition model is then, given an input image, is to make a prediction, i.e. output a result. This result is then compared to the target output (i.e. the correct label) for each input image. . How much the model output is right or wrong for each input (for classification tasks), or how far the model is wrong or right for a continuous value (for regression tasks) is known as the loss. The loss is usually (but not necessarily always) computed in such a way that the more accurate or better the model is in making a prediction, the smaller the loss. . This function that measures the difference between the target output versus the output predicted by the model is known as the loss function. So we can say that the loss function which the one that produces a measure of a model’s performance. . Once a loss is computed (this is a numerical value that indicates how well a model is doing its task, usually the smaller the better), the next step is to improve or optimize the model’s performance by adjusting the weights. . How it does that computation of the adjustment is done by an algorithm known as Gradient Descent, which is one of the secret ingredients (if ever there was one) behind the power of neural networks. . Supervised Learning and Other Types of Learning . Because this machine learning process uses a set of labelled data (associated with each input data is the target result or label, whether this is a category or a continuous number) which used to check the models predictions if they correct or not (in other words, the labels are used to supervise whether the model is behaving correctly), it is called supervised learning. . There are other methods that don’t use labels as such – as in unsupervised learning where the model uses unlabeled data to extract patterns that maybe useful in classifying the data. Another approach is called semi-supervised or self-supervised learning where the labels maybe embedded in the input data itself. . An example of this is in Natural Language Processing (NLP) where models might use text from Wikipedia and other sources as the input data and the model is trained such that it can extract the patterns from the underlying language, for example, in language generation, such that given a sequence of words, the model is able to predict the next word or next sets of words that will appear in a sentence or paragraph. The “labels” as such are embedded or latent in the data of the text itself, so while there is no explicit label, the goal is to create a model, given a start of a sentence, might be capable of being able predict what the missing word or words might be. So the goal or loss function might be to improve the accuracy of the prediction and therefore a model can be trained this way, despite the input data not having explicit labels associated with it. . Another area of machine learning is Reinforcement Learning, where a model takes on a task such as playing a video game or Go or chess, where the loss function is to minimize its losses (and/or maximize its wins). And by playing against a copy of itself, the model learns what strategies leads to success or failures and keeps improving and improving until, for example, it could beat their human counterparts. . This article will be focused primarily on supervised learning as this is the basic building block for all the other methods. . Gradient Descent . As stated previously, Gradient Descent is the process that models use in order to adjust their parameters towards some goal. . We can think of the neural network as a whole (even though it is actually composed of layers and layers of neurons) as actually just being a function. It is a function that outputs a y given a set of inputs x. Moreover, it outputs the y based on input x and a set of parameters w (weights) and b (biases). . So given a fixed set of inputs x (e.g. a set of N samples of images), modifying the parameters w and b slightly will cause the model to output a slightly different result and hopefully a change in the loss. The change in the loss (delta of the loss) resulting from a slight change of the parameter (delta of the parameter) is known as the gradient. For linear functions, this gradient is also known as the slope (or rise/run). In other words, a gradient for a parameter tells us how changing that parameter (which may number in the thousands for a typical model) affects the loss. . Once we know the gradients of the parameters, we can then adjust the parameters in such a way as to reduce the loss (in other words, improve the performance). . Gradient Descent works by computing the gradients of parameters p with respect to a given set of inputs x and its computed outputs yhat , the target outputs y and the resulting loss (a math like jargon definition) . Each parameter is then updated (reduced or increased) by an amount equal to the gradient of that parameter multiplied by a factor known as the Learning Rate. . Then the model is tested again, by computing the output yhat given the input x and the adjusted parameters p. The loss is then again computed based on the adjusted parameters. If done correctly, the loss should now be smaller and the model improves its performance. Looping through this process (and keeping track of its performance) again and again, will theoretically, eventually result in a model that can be good enough for the targeted task. . The process by which a model computes a loss using a loss function, computes the gradients of the parameters with respect to that loss, and the update of the parameters in order to improve its performance is known as optimization and is done by another component (not part of the model) known as an optimizer. . The process of computation of the gradients is known as backpropagation. . The process of the neurons’ computation of the inputs into their outputs and subsequent input to the next layer of neurons all the way through the layers to output a predicted result for the model is known as forward propagation. . One important function that needs to happen during forward propagation phase is that the framework that is used to implement the neural network models needs to keep track of the calculations being done by the neurons on the parameters, the activation functions all across the layers of the model up to the calculation of the loss. . This sequence of calculations, or more accurately, graph of calculations is known as the model’s computational graph. This computational graph is then used by the framework to compute the parameters’ gradients in the backpropagation phase of the training. . When the trained model is used to compute a prediction based on a new set of inputs (such as in model deployed in production), this forward propagation is also called inference. Normally, during inference, the model is still forward propagation, but the framework no longer needs to create a computational graph because it doesn’t need to compute the gradients of the parameters – the gradients are only needed when we plan to update the parameters, but computing the output does not need the gradients, and are usually turned off. . Stochastic Gradient Descent . In the process we described above, we assumed our computation of loss (and the subsequent computation of the gradients) based on all the samples of the training data once every epoch (where an epoch is one pass through all training data). . In practice, this is very hard to do, due to limitations of GPU hardware (usually memory) in which the operations to compute the predictions, computation of the loss, computation of the gradients, and update of the parameters are all done by a GPU so these pieces of data need to be stored in GPU memory which might be less than the memory available for the CPU. . The reason we have to use a GPU is because the number of parameters can be very large and while the operations might be simple (multiplication, addition, differentiation (aka computation of the gradients) - even differentiation is just multiplication and adddition), there needs to be lots of them, and GPUS are ideal for this soft of computation because they can be parallelized. . One alternative to computing the loss for the all the samples at once, is to compute the loss (and the subsequent gradients) for each sample of input data (e.g. one image at a time) and update the weights as we pass through all the samples. In this extreme case, an epoch (where an epoch is one pass through all your training data) consists of N passes of the training loop given N samples of training data. This is known as online or sequential gradient descent and is considered a variant of stochastic gradient descent. . A more common alternative is the middle ground where we take a batch of n input samples at a time (call this batch size bs) and compute the loss, compute the gradients and adjust the parameters for the entire batch. This means that as the model goes through each batch, the model is computing the loss from each batch and applying the update to the model’s parameters to be used for the predictions of the next batch to see if its update does result in a lower loss (i.e. improve the model’s performance). . So for a given batch size bs, there will be m batches where m is equal to N input samples divided by the batch size bs. This means that for each epoch, there will be m passes through the training loop. This is known as batch gradient descent and is also considered as a variant of stochastic gradient descent. . This can maximize the utilization of the GPU as we can adjust the batch size so that the entire input data for each batch plus the parameters can fit into the GPU memory. . Also, we can also consider online or sequential gradient descent as a batch gradient descent with a batch size of one. . As an aside, Stochastic means randomly determined. The reason why the we call the online and batch gradient descent stochastic is because they replace the computation of the loss of the entire input data with a stochastic (random) approximation (based on a sample subset of the input data). Moreover, as the same input data is read passed through each epoch, the contents of each batch are usually shuffled, resulting in a set of parameters that can better approximate the parameters for the entire input data through random sampling. . Hyperparameters . The learning rate, the batch size and the number of epochs are the tweaks in the way we train our model, and along with lots of others (including the architecture), are known as hyperparameters. Learning to pick what good values to set these to will, (along with how good your training data is, as well) determine how quickly or how well we can build a model to perform a target task. . Unfortunately, at this point, this process of finding the right hyperparameters is as much as an art and a skill that can only be developed through experience, although there might be some rules of thumb that we can follow. Often, the only way to know is by trying things out and is often a source of challenge as much as a source of frustration for deep learning practitioners. . Linear Functions and Activation Functions . The most common function used in the neurons that compose the layers of neural networks is the linear function wx + b, where x represents the inputs, w the weights and b is the bias. In order for a neuron (or a layer of neurons) to implement Gradient Descent, it is important that its function be differentiable, meaning a gradient can be computed on the parameters w and b given a loss function that computes the output of the last layer of the model. . It is also important to note that for each layer in the neural network , the input x might be a set of numbers, e.g. a vector or a matrix of numbers, and that the weights can also be matrices and biases can also be vectors, so a more accurate depiction of the linear equation is w@x + b where @ represents a matrix multiplication. . An alternate formulation of the Linear function (and an explanation for usage of the word bias) . As an alternate formulation, the linear equation w@x + b can also be expressed as w@x, but with the slight modification of the x inputs – the addition of an input(possible a vector) = 1 and the weights w incorporating the bias at the end of the matrix. So the inputs always include an additional input that is always equal to 1 while the weights now incorporate the bias as the last column. multiplying the input of 1 with the weight b is equal to b – so this is equivalent to w@x + b. The reason why it is called bias is that a bias is a term used in electrical circuits for a component that raises the output to a constant value - even when the rest of the inputs are zero. The advantage of this formulation is that it simplifies differentiation (i.e. the computation of the gradients, since everything that needs to be differentiated is in the matrix w, including the bias, albeit at the expense of having to add an extra input value equal to 1 during the computation of the prediction) . Activation Functions . Activation functions on the other hand are transformations between the outputs of one layer to the inputs of the next layer. Their function is to introduce a non-linearity between the linear functions of the neurons. . Mathematically, tying together the output of a linear function to the input of another linear function is actually equivalent to just another linear function. So if the neural network was simply composed of layers of neurons computing linear equations linked directly (i.e. their output was fed directly) to the next layer of neurons also computing linear equations down the line, it would simply be equivalent to a single layer of neurons computing linear equations. And neural networks composed only of one layer are not going to be able to do the feats that deep learning neural networks have become famous for. . An example of a non-linear function is the ReLU or rectified linear unit. In practical terms, its just a function that replicates (and outputs) the input if the input is greater than zero, otherwise, it just outputs zero. In other words, it just zeroes out the negative outputs and passes on the positive outputs. . Now, this may not sound such a radical transformation, but this, along with the Gradient Descent, allows any neural network to theoretically approximate any function, given a set of inputs. This capability of neural networks to approximate any computation for a given set of inputs is known as the Universal Approximation Theorem. If anything, activation functions are the other secret ingredient behind the power of neural networks. . Loss functions . An important characteristic of a good loss function is that it should be sensitive to changes in the parameters, i.e. slight changes to the parameters should also change the computed loss. This is so that when we differentiate the loss function and compute the gradients of the parameters, the change in the loss will result in non-zero values for the gradients. This, in turn, will trigger a change in the parameters (because they are adjusted by an amount equal to the value of the gradient multiplied by a factor known as the learning rate) for the iteration of the training loop. . If a loss function was not sensitive to changes in the parameters (i.e. slight changes in the parameter results in the same value of loss), then gradients computed would be zero and so the adjustment would also be zero (zero gradient times any value of learning rate is also zero) and would end up in no change in the parameters (ad infinitum). At this point, the model would no longer be improving – another way of saying that the model is no longer learning. . We can visualize the all possible parameter values as occupying a multi-dimensional space and the loss function maps each point in this space to a numerical value (given a fixed set of inputs). Moving slightly from one point in this parameter space to another point results in another value for the loss. A bad loss function is one with lots of flat areas (i.e. zero gradients) while good loss functions would have hills and valleys leading down to (hopefully) a global minimum or at least something close to it. . The goal of training the model is to find a multidimensional point in this parameter space (in other words the set of parameters ws and biases for the entire model) that computes a minimum loss for the entire training data. . Metrics . While we’ve already talked a little bit about loss functions and how they’re used to improve the model using gradient descent, from a practical perspective, what we really care about a model’s performance is known as its metrics. . So a metric is also a measurement of the performance of a model, but unlike the loss function, it doesn’t have to be differentiable, it just has to be a reasonable measure of performance from the practitioner’s perspective. Sometimes you can use the loss as the metric, but you might be able to use something more appropriate and understandable. . One of the most common metrics (especially for classification) we use is error rate - given a sample of N inputs, how many mistakes (or wrongly classified) M did it make, and the error rate is M divided by N. The lower it is, the better. Zero error rate means no mistakes. As an alternative, we can also take accuracy – which is just 1 - error rate (or how many correct responses did the model make given N input samples). . For regression tasks (where the model is predicting a continuous value), the MSE or Mean Squared Error or its square root (aka RMSE) can be used as a metric. Note that in this case, the MSE can also be used as the loss function. . Overfitting and Validation Sets . The next question is to what set of samples do we apply the metrics to? If we simply use the training data, it is very easy to get zero error rates (usually). But when we actually try to use the model in production, we might get disappointed and not get the same performance (metric wise) that we got during training. . The reason for this might be that our model has been optimized to such an extent that its has become so specialized so as to match the training data that it is no longer performs as well on data that you use during production. . The model might have seen your training data so many times that it has sort of “memorized” it and can give good predictions on it but it won’t necessarily perform well on data it hasn’t seen during training. This situation where the model performs (both the loss and your metrics) well on your training data but doesn’t do so well on data it didn’t see during training is known as overfitting. . In order to counter this, and to also have a better estimate of the model’s performance when we deploy it during production, we can simulate this set of data we will encounter during production by setting aside some of our labelled data that we would normally use for training the model to be used to validate its performance. This set of data is what we call our validation data, as distinct from the training data. . This validation data is where we apply our metrics in order to get an estimate of the performance of our model when we deploy it in production. These metrics are much less useful when we apply them to our training data because they won’t give a good estimate of the model’s performance when used in production (often, they inflate the performance and give us a false sense of confidence). This is why we mostly track our metrics against the validation data, not on the training data. . Also, it is also very important to note that our training and validation data that we use for training the model as well as estimating its performance should be as representative as possible of the data that we will actually encounter when we deploy the model into production. . This is important for the training data, as the model can only learn based on the input data that it encounters, and may not perform as well on data that is very different that it may encounter during production. . This is also important for the validation data, because if the validation data is not representative of the data it will encounter in production, we might have a wrong estimate the performance of the model when it is actually used in production if we base it on the metrics we measure on the validation data. . The last point we need to make is that we need to split the validation data from the training data in such a way that the data we use in the validation set is representative of the data we will encounter in production. So for example, if in production, we will have images of cats and dogs that we didn’t have in our training data, then our validation data should also have images of cats and dogs that are not part of our training data. . If didn’t do this, and we use the same images in both our training and validation data, then its quite possible that we would score high on our metric (because it may have “memorized” the training data) but most certainly perform worse when we actually use it in production. . One way to approach this splitting of training and validation data is to make sure that our validation data is not “leaking” into our training data. If, for example, our samples that are in the validation data can somehow be correlated into samples that are also in the training data in a way that will never happen in production, then that might constitute a “leakage”. What constitutes this leakage is of course very dependent on model’s task and the data we use to train it. . Using the metrics to improve the model . If we do the training-validation split correctly, then our validation metrics should be a good predictor of the model’s performance when it is deployed in production. As such, we can use the metrics to guide us on tweaking the model until its metrics are good enough or even tell us to stop if the metrics start to worsen. We can also use it to compare different models and allow us to pick the one with the best performance. . As we learn different techniques and learn different “hyperparameters” to tweak the model, at some point, that although the model does not use or “see” the validation data during its training, we, as the practitioners guiding the process of selecting hyperparameters towards a set of the model’s parameters that result in a good performance metric on the validation data, we might have actually started “overfitting” the model to match the validation data, at the expense of worsening its performance on data it will encounter once we deploy it in the real world. Though less of a possibility, it is still a possibility. . Test Datasets . In order to counter this “leakage” of the validation data into the model, one possible solution is to set aside a third set of data known as the test data. This is again split from the labelled data and the usual caveats apply, i.e. it must be representative of the data the model will encounter in the real world and must not be correlated with samples present in the training and validation data. This test data will not be used in the training process and when we are updating the hyperparameters and validating the performance metrics using the validation dataset. It will only be used to predict the performance of the model once all the tweaking has been done. . Tracking the performance of the model during training . As we go through each epoch (each epoch is a complete pass through all your training data), and since we are using batch gradient descent where for each epoch we have subdivided your training data into batches, we will be executing the training loop for each batch and updating the model’s parameters each time we pass through the training loop. . In order to give us an idea of the current performance of the model, at the end of the training loop for all batches, we can then compute the metrics and loss on the validation dataset, doing the inferencing (making sure we do this without updating the computational graph since only the forward propagation steps on the training data should be updating the computational graph since its the gradients on the training input data that should be used to update parameters). . The computation of the validation loss and metrics are also done in batches using the GPU because they run quicker if they are parallelized, but usually the batch sizes are double the batch sizes for the training phase because they dont use the computational graph to track the calculations on the inference and loss calculations. At the end of all the validation batches, the means of the validation loss and metrics are displayed along with the training loss to give an indication of the model’s performance. . Parameter Initialization and Transfer Learning . In the section on training the model, especially for the cat-dog image recognition example, we did some hand waving on how we actually get a starting point for the model. . Remember, we can usually define an architecture (which is how the many neurons there are in each layer, how many layers and how each layer connects to the other layers), but we haven’t discussed how we come up with the initial values for the parameters themselves. . If we didn’t have any other starting point, the way practitioners usually set the initial parameters are by setting them to random values. . Of course, there are also particularities in the way we setup these random parameters but the idea is that whatever random values they start with, during the training phase, these parameters converge to a set of values that will provide the optimum level of performance needed to perform a task. . As an alternative to starting out with random values, we could also use a model trained on a similar task but not necessarily the same as the task we want the model to optimize. This idea of using an pre-existing pre-trained model and adapting it to your particular task is known as transfer learning. . Transfer learning, if done with the appropriate pre-existing pre-trained model, can reduce the amount of data and computation needed to reach an optimum level of performance. . This is the primary reason why, by using transfer learning, we can reach state of the art performance even on limited computing requirements and limited amounts of labelled data. . For computer vision tasks, these pre-trained image recognition models usually come in a set of well defined architectures that have been known to perform well in some competititon and have usually been trained on large image datasets. . Copyright &copy; 2020 by Butch Landingin. All rights reserved. version 0.1.11 . . Footnotes . Feynman Method &#8617; . |",
            "url": "https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/04/27/machine-learning-foundations.html",
            "relUrl": "/machine%20learning/2020/04/27/machine-learning-foundations.html",
            "date": " • Apr 27, 2020"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://butchland.github.io/butchland-machine-learning-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}