<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://butchland.github.io/butchland-machine-learning-notes/feed.xml" rel="self" type="application/atom+xml" /><link href="https://butchland.github.io/butchland-machine-learning-notes/" rel="alternate" type="text/html" /><updated>2020-10-05T11:55:40-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/feed.xml</id><title type="html">Butch Landingin’s Machine Learning Notes</title><subtitle>My explorations in machine learning</subtitle><entry><title type="html">Build (and Run!) Your Own Image Classifier using Colab, Binder, Github, and Google Drive (Part 2)</title><link href="https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/10/05/byoic-on-colab-part2.html" rel="alternate" type="text/html" title="Build (and Run!) Your Own Image Classifier using Colab, Binder, Github, and Google Drive (Part 2)" /><published>2020-10-05T00:00:00-05:00</published><updated>2020-10-05T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/10/05/byoic-on-colab-part2</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/10/05/byoic-on-colab-part2.html">&lt;p&gt;&lt;img src=&quot;https://imgs.xkcd.com/comics/trained_a_neural_net.png&quot; alt=&quot;trained a neural net&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;building-your-own-image-classifier-is-fun&quot;&gt;Building your own image classifier is fun!&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;This is Part 2 of a two-part article on building your own image classifier.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you haven’t read the Part 1 yet, please read it first as this second part continues where the Part 1 left off.&lt;/p&gt;

&lt;p&gt;Here’s the &lt;a href=&quot;/butchland-machine-learning-notes/machine%20learning/2020/09/21/byoic-on-colab.html&quot; target=&quot;_blank&quot;&gt;link to Part 1&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sharing-your-app-to-the-world&quot;&gt;Sharing your app to the world&lt;/h2&gt;

&lt;p&gt;In order to share it to the world, we will be using &lt;a href=&quot;https://mybinder.org&quot; target=&quot;_blank&quot;&gt;Binder&lt;/a&gt; , another cloud based service that can run Jupyter notebooks as applications (much like Colab can also run Jupyter notebooks).&lt;/p&gt;

&lt;p&gt;The difference is that Colab is an interactive editor environment (allowing you to edit and run your Jupyter notebooks) while Binder is just for running your Jupyter notebooks as applications.&lt;/p&gt;

&lt;h2 id=&quot;create-a-github-repo&quot;&gt;Create a Github Repo&lt;/h2&gt;

&lt;p&gt;In order to run your Jupyter notebook on Binder, we will need to create a Github repository (also known as a &lt;em&gt;repo&lt;/em&gt;), which is a place to store the associated files necessary to run your app.&lt;/p&gt;

&lt;p&gt;This includes the exported image classifier named &lt;code class=&quot;highlighter-rouge&quot;&gt;export.pkl&lt;/code&gt; file which we built earlier.&lt;/p&gt;

&lt;p&gt;If you haven’t done so earlier, you’ll need to create a Github ID, which grants you access to the Github site and allows you to create your own repos.&lt;/p&gt;

&lt;p&gt;Click on &lt;a href=&quot;https://github.com/join?source=header-home&quot; target=&quot;_blank&quot;&gt;the link here&lt;/a&gt; to create a Github ID.&lt;/p&gt;

&lt;p&gt;Once you’ve created a Github ID, you can now create repos on Github.&lt;/p&gt;

&lt;p&gt;Click on &lt;a href=&quot;https://github.com/butchland/build-your-own-image-classifier-template/generate&quot; target=&quot;_blank&quot;&gt;this link here&lt;/a&gt; to generate your repo.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Make sure to change the name of project to something that is &lt;strong&gt;NOT&lt;/strong&gt; the same name as original name &lt;code class=&quot;highlighter-rouge&quot;&gt;build-your-own-image-classifier-template&lt;/code&gt;)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is to work around a limitation of Github regarding public forks. Naming your project &lt;code class=&quot;highlighter-rouge&quot;&gt;build-your-own-image-classifier&lt;/code&gt; (without the &lt;code class=&quot;highlighter-rouge&quot;&gt;-template&lt;/code&gt;) is fine, but naming your project to something unique for yourself would be better.&lt;/p&gt;

&lt;p&gt;Please note down the name of your Github ID, your Github password as well as the name of your repository (or &lt;em&gt;repo&lt;/em&gt; for short) as you will use them in the next steps below.&lt;/p&gt;

&lt;h2 id=&quot;upload-your-exported-image-classifier-file-to-github&quot;&gt;Upload your exported image classifier file to Github&lt;/h2&gt;

&lt;p&gt;Once you’ve created your Github repo, you are now ready to &lt;em&gt;“push”&lt;/em&gt; or copy the exported image classifier from your Google Drive to your Github repo.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you’re wondering why can’t we just download the exported image classifier from Colab and upload it to your Github repo – it’s because of a limitation of Github which restricts uploads of files bigger than 25MB. Unfortunately, your exported image classifier file (&lt;code class=&quot;highlighter-rouge&quot;&gt;export.pkl&lt;/code&gt;) is usually bigger than this so we have to use &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt;  - a version control system used by Github, to move the exported image classifier from Colab to our Github repo)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Click on the button below to run another notebook on Colab which will do this.&lt;/p&gt;

&lt;p&gt;Fill out your github repo, github id and password as explained in the notebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/butchland/build-your-own-image-classifier/blob/master/colab-export-image-classifier.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once you have done this, your exported image classifier should now be visible in the list of files of your Github repo.&lt;/p&gt;

&lt;p&gt;Your Github repo URL should have the format &lt;code class=&quot;highlighter-rouge&quot;&gt;https://github.com/&amp;lt;your-github-id&amp;gt;/&amp;lt;your-github-repo-name&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/butchland/build-your-own-image-classifier/master/images/add-export-pkl-to-repo.png&quot; alt=&quot;check export.pkl in list of repo files image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please check that the &lt;code class=&quot;highlighter-rouge&quot;&gt;export.pkl&lt;/code&gt; file is in the list of files of your repo.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If it’s not there, then something went awry with the previous step – maybe your github id, github repo or password were entered incorrectly. You might need to rerun the previous step above.)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;run-your-app-on-binder&quot;&gt;Run your app on Binder&lt;/h2&gt;

&lt;p&gt;In order to run Binder, please take note of url of the Github repo you created in the previous step.&lt;/p&gt;

&lt;p&gt;Next, head over to Binder by &lt;a href=&quot;https://mybinder.org&quot; target=&quot;_blank&quot;&gt;clicking here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Configure Binder as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/butchland/build-your-own-image-classifier/master/images/binder-launch-screen.png&quot; alt=&quot;binder main page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the &lt;code class=&quot;highlighter-rouge&quot;&gt;GitHub repository name or URL&lt;/code&gt; field (marked with the text 1), paste the url of your Github repo.&lt;/p&gt;

&lt;p&gt;In the &lt;code class=&quot;highlighter-rouge&quot;&gt;Path to a notebook file (optional)&lt;/code&gt; field (marked with the text 2), enter &lt;code class=&quot;highlighter-rouge&quot;&gt;/voila/render/build-your-own-image-classifier.ipynb&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Change the &lt;code class=&quot;highlighter-rouge&quot;&gt;path type&lt;/code&gt; (marked with the text 4 right next to the &lt;code class=&quot;highlighter-rouge&quot;&gt;Path to a notebook file (optional)&lt;/code&gt; field) from &lt;code class=&quot;highlighter-rouge&quot;&gt;File&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;URL&lt;/code&gt;.&lt;em&gt;(This will change the label of the previous field to &lt;code class=&quot;highlighter-rouge&quot;&gt;URL to open (optional)&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Make sure to fill out the three fields correctly.&lt;/p&gt;

&lt;p&gt;Once done, click on Orange-Yellow “Launch” button (located right next to the &lt;code class=&quot;highlighter-rouge&quot;&gt;URL to open (optional)&lt;/code&gt; field) and you are all set.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Note that starting your application the first time will be slow, as Binder must first assemble your application into a ready-to-run format. The next time your application starts, it will start up a little bit quicker.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note down the Share URL to your app (marked with text 3 in the above image) so you can share it with your friends and family to explore.&lt;/p&gt;

&lt;h2 id=&quot;copy-and-share-your-binder-app-link&quot;&gt;Copy and Share your Binder App Link&lt;/h2&gt;

&lt;p&gt;When you run your app on binder, Binder provides a link to rerun your app. Save the link and share that link to your friends and family so they can run your application.&lt;/p&gt;

&lt;h2 id=&quot;profit&quot;&gt;Profit!!&lt;/h2&gt;

&lt;p&gt;If you followed the steps above, you now should have image classifier running on Binder so its time for &lt;strong&gt;Profit&lt;/strong&gt;!!&lt;/p&gt;

&lt;h2 id=&quot;what-we-learned&quot;&gt;What we learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;That building image classifiers doesn’t require you to have deep pockets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;That building image classifiers doesn’t require a PhD &lt;em&gt;(not that having one is bad, just not required for mastering Deep Learning)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It also doesn’t require lots of math &lt;em&gt;(maybe some, but not a lot)&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;want-to-learn-more&quot;&gt;Want to learn more?&lt;/h2&gt;

&lt;p&gt;If you want to learn more (we’ve just scratched the surface), take this totally free course (no Ads!) called &lt;a href=&quot;https://course.fast.ai&quot; target=&quot;_blank&quot;&gt;Practical Deep Learning for Coders&lt;/a&gt;  – it will teach you Deep Learning from top to bottom, using running code, on Jupyter notebooks like the ones we used on Colab, and build useful applications beyond the simple image classifier we built here.&lt;/p&gt;

&lt;p&gt;You can also join an &lt;a href=&quot;https://forums.fast.ai&quot; target=&quot;_blank&quot;&gt;inclusive global learning community&lt;/a&gt;  that welcomes beginners and experts alike and ready to help you start your deep learning journey.&lt;/p&gt;

&lt;p&gt;Hope to see you there!&lt;/p&gt;

&lt;h2 id=&quot;final-note-and-acknowledgements&quot;&gt;Final Note and Acknowledgements&lt;/h2&gt;

&lt;p&gt;The software used to clean your data, build your classifier and run your application is largely based on the &lt;a href=&quot;https://docs.fast.ai&quot; target=&quot;_blank&quot;&gt;fastai&lt;/a&gt; python package, written by Jeremy Howard and Sylvain Gugger.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://jupyter.org/&quot; target=&quot;_blank&quot;&gt;Jupyter Notebook&lt;/a&gt; environment that enables us to run interactive documents to build our image classifier won the 2017 ACM Software System Award and is widely used by scientists, researchers and students around the world.&lt;/p&gt;

&lt;p&gt;The software package used to collect the images (&lt;a href=&quot;https://joedockrill.github.io/jmd_imagescraper/&quot; target=&quot;_blank&quot;&gt;jmd_imagescraper&lt;/a&gt;) was built by Joe Dockrill (&lt;a href=&quot;https://forums.fast.ai/u/joedockrill&quot; target=&quot;_blank&quot;&gt;@joedockrill&lt;/a&gt;), one of the students of the fast.ai course.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://course.fast.ai&quot; target=&quot;_blank&quot;&gt;fast.ai course &lt;strong&gt;Practical Deep Learning for Coders&lt;/strong&gt;&lt;/a&gt; is considered one of the best introductory courses on Deep Learning, which is taught by Jeremy Howard and Rachel Thomas, who are also the founders of &lt;a href=&quot;https://fast.ai&quot; target=&quot;_blank&quot;&gt;fast.ai&lt;/a&gt;, a non-profit organization working towards democratizing the use of AI in the world.&lt;/p&gt;

&lt;p&gt;There’s also a book &lt;em&gt;“Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD”&lt;/em&gt; &lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot; target=&quot;_blank&quot;&gt;available on Amazon&lt;/a&gt;, but the authors (Sylvain and Jeremy) have made the &lt;a href=&quot;https://github.com/fastai/fastbook&quot; target=&quot;_blank&quot;&gt;whole book available as Jupyter notebooks&lt;/a&gt; for free as well.&lt;/p&gt;

&lt;p&gt;The steps to run on Binder were based on a &lt;a href=&quot;https://forums.fast.ai/t/deploying-your-notebook-as-an-app-under-10-minutes/70621?u=butchland&quot;&gt;forum post&lt;/a&gt; created by Vikrant Behal (&lt;a href=&quot;https://forums.fast.ai/u/vikbehal&quot; target=&quot;_blank&quot;&gt;@vikbehal&lt;/a&gt;), another student of the fast.ai course.&lt;/p&gt;

&lt;p&gt;If you liked this article, and most specially if you were able to build your own classifier based on this article, give me a shoutout on &lt;a href=&quot;https://twitter.com/butchland&quot; target=&quot;_blank&quot;&gt;my twitter account @butchland&lt;/a&gt; or message me on the &lt;a href=&quot;https://forums.fast.ai/u/butchland&quot; target=&quot;_blank&quot;&gt;fastai forums: @butchland&lt;/a&gt;, I’d really appreciate it!&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://butchland.github.io/butchland-machine-learning-notes/images/image-classifier-logo.png" /><media:content medium="image" url="https://butchland.github.io/butchland-machine-learning-notes/images/image-classifier-logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Build (and Run!) Your Own Image Classifier using Colab, Binder, Github, and Google Drive</title><link href="https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/09/21/byoic-on-colab.html" rel="alternate" type="text/html" title="Build (and Run!) Your Own Image Classifier using Colab, Binder, Github, and Google Drive" /><published>2020-09-21T00:00:00-05:00</published><updated>2020-09-21T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/09/21/byoic-on-colab</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/machine%20learning/2020/09/21/byoic-on-colab.html">&lt;p&gt;&lt;img src=&quot;https://imgs.xkcd.com/comics/trained_a_neural_net.png&quot; alt=&quot;trained a neural net&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;building-your-own-image-classifier-is-fun&quot;&gt;Building your own image classifier is fun!&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;This is Part 1 of a two-part article on building your own image classifier. Here’s the &lt;a href=&quot;/butchland-machine-learning-notes/machine%20learning/2020/10/05/byoic-on-colab-part2.html&quot; target=&quot;_blank&quot;&gt;link to Part 2&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So here’s what were building – &lt;strong&gt;A pet classifier!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, head over to Binder to see it in action – click on the  button below &lt;em&gt;(where it says “launch binder”)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Play it with for a while (make sure you upload some photos). Check out how well (or how bad) it classifies your pets and come back to this page to continue.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mybinder.org/v2/gh/butchland/automatic-garbanzo/master?urlpath=%2Fvoila%2Frender%2Fbuild-your-own-image-classifier.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Binder pet classifier&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: it takes a while for it to start, so please be patient, I promise it will be worth the wait!&lt;/p&gt;

&lt;h2 id=&quot;whats-an-image-classifier&quot;&gt;What’s an image classifier?&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;I hope you clicked on the “launch binder” button above and played with the pet classifier app before coming back here to continue reading the article.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you did play around with app above &lt;em&gt;(like I told you to)&lt;/em&gt;, you’d know that an &lt;em&gt;image classifier&lt;/em&gt; classifies images.&lt;/p&gt;

&lt;p&gt;This article will show you how to build your own image classifier – by default, you’ll be building a &lt;em&gt;pet classifier&lt;/em&gt; just like in the demo above, but you can tweak it so you can build an image classifier for whatever you want &lt;em&gt;(as long as you can find enough pictures of it to serve as examples)&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;some-prerequisites&quot;&gt;Some prerequisites&lt;/h2&gt;

&lt;p&gt;Because we’re using cloud services to build everything in this project (no need to install anything on your computer), you’ll have to signup for some services, so you’ll need to have these.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A Gmail ID &lt;em&gt;(almost everyone has one these days, don’t they? But if you don’t, head over to &lt;a href=&quot;https://accounts.google.com/signup/v2/webcreateaccount?hl=en&amp;amp;flowName=GlifWebSignIn&amp;amp;flowEntry=SignUp&quot; target=&quot;_blank&quot;&gt;this link here&lt;/a&gt; right now and create one.)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Github ID &lt;em&gt;(don’t worry if you don’t have one, we can create it later – but if you’re in a hurry, you can create one right now – just click on &lt;a href=&quot;https://github.com/join?source=header-home&quot; target=&quot;_blank&quot;&gt;this link here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;steps-to-building-your-own-image-classifier&quot;&gt;Steps to building your own image classifier&lt;/h2&gt;

&lt;p&gt;So now we’re ready to start building!&lt;/p&gt;

&lt;p&gt;Here are the steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Collect some pictures&lt;/em&gt; – if you don’t have pictures, you can usually search for them over the internet.&lt;/p&gt;

    &lt;p&gt;We will be using &lt;a href=&quot;https://colab.research.google.com/&quot; target=&quot;_blank&quot;&gt;Colab&lt;/a&gt;, a Google cloud service used by data scientists and machine learning researchers and students around the world to build and study machine learning models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once you’ve collected your pictures, you’ll need to &lt;em&gt;clean your data&lt;/em&gt; – you’ll see that the images you get through an internet search are sometimes incorrect, so you need to clean up your data first if you want your image classifier to work well – after all, like in a lot of things, if your image classifier gets &lt;strong&gt;Garbage in&lt;/strong&gt;, it puts &lt;strong&gt;Garbage out&lt;/strong&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now that you have cleaned your data, you can build a &lt;em&gt;Neural Network Image Classifier&lt;/em&gt; that can classify images.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;(Neural Networks a.k.a. Deep Learning Artificial Neural Networks are currently the best way to build image classifiers – that’s why we are using them!)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once you have your image classifier, you need to export it to a format so that it can be used just like a regular program – you feed it an image, it will classify that image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We will also build a very simple app that can use that exported image classifier.&lt;/p&gt;

    &lt;p&gt;We can test it out on Colab, but in order to make it available for others to use, we’ll need to use a cloud platform that can run your app.&lt;/p&gt;

    &lt;p&gt;In our project, we will be using &lt;a href=&quot;https://mybinder.org/&quot; target=&quot;_blank&quot;&gt;Binder&lt;/a&gt;, a free cloud service that can run your image classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next, you’ll need to move that exported image classifier to &lt;a href=&quot;https://github.com/&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; (along with your app) because that’s where Binder will get it from in order to run it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once you get your exported image classifier on Github, you can now run your image classifier app on Binder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The next step is … &lt;strong&gt;Profit!&lt;/strong&gt; – as the whole world starts using your app… (or maybe just your friends :)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Don’t worry if you haven’t figured out what to do next, I’m just outlining the steps of the whole process, but I’ll be walking you through the process step by step.&lt;/p&gt;

&lt;p&gt;I’ll even try to explain what we’re doing as we’re doing it, but even if you just follow the instructions (without understanding) you should just do fine (but understanding what you’re doing makes it more fun, doesn’t it?)&lt;/p&gt;

&lt;h2 id=&quot;collecting-images&quot;&gt;Collecting Images&lt;/h2&gt;

&lt;p&gt;In order to build a Deep Learning Image Classifier, we need data.&lt;/p&gt;

&lt;p&gt;This data (in the form of labeled pictures) will be used as examples from which the Neural Network learns to distinguish between different categories.&lt;/p&gt;

&lt;p&gt;In our case, our Neural Network Image Classifier distinguishes cats from dogs.&lt;/p&gt;

&lt;p&gt;So we will need pictures of cats and dogs. Not only do we need pictures of cats and dogs, we will need to label each picture as containing either a cat or a dog.&lt;/p&gt;

&lt;h2 id=&quot;introducing-colab&quot;&gt;Introducing Colab&lt;/h2&gt;

&lt;p&gt;As previously stated, we will be using &lt;a href=&quot;https://colab.research.google.com/&quot; target=&quot;_blank&quot;&gt;Colab&lt;/a&gt; in order to do this.&lt;/p&gt;

&lt;p&gt;So you might be wondering, what is Colab and how do I use it?&lt;/p&gt;

&lt;p&gt;Colab is an interactive document editor, much like Google Docs, but unlike Google docs, you can run code inside it.&lt;/p&gt;

&lt;p&gt;These Colab documents are called Jupyter notebooks, and they are stored in your Google Drive, just like Google Docs, Sheets or Slides.&lt;/p&gt;

&lt;p&gt;Jupyter notebooks contain both prose (such as text, graphs, and images) as well as code.&lt;/p&gt;

&lt;p&gt;You can also download them to your local drive (with the file extension &lt;code class=&quot;highlighter-rouge&quot;&gt;.ipynb&lt;/code&gt;) or store them on &lt;a href=&quot;https://github.com/&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;, which is a cloud-based public repository for code and documents.&lt;/p&gt;

&lt;p&gt;I’ve built Jupyter notebooks for each step of building your own image classifier, so we can just run them one after another until we’ve built our image classifier.&lt;/p&gt;

&lt;p&gt;The nice thing is, because Jupyter notebooks contain both text and code, I’ve put in explanations about what the code is doing alongside the actual code (kind of like code comments, but better formatted.)&lt;/p&gt;

&lt;h2 id=&quot;running-your-first-notebook-to-collect-images&quot;&gt;Running your first notebook to collect images&lt;/h2&gt;

&lt;p&gt;Click on the button (where it says “Open in Colab” ) below to open a copy of the notebook.&lt;/p&gt;

&lt;p&gt;If you haven’t already logged in to Google, you’ll have to login first.&lt;/p&gt;

&lt;p&gt;Follow the instructions in the notebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/butchland/build-your-own-image-classifier/blob/master/colab-build-image-dataset.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/butchland/build-your-own-image-classifier/master/images/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At the end of the steps in that notebook, you should have a labeled image dataset for your image classifier stored on your Google Drive.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you didn’t modify the default project name, the file &lt;code class=&quot;highlighter-rouge&quot;&gt;pets.tgz&lt;/code&gt; should be in your Google Drive under the folder &lt;code class=&quot;highlighter-rouge&quot;&gt;/My Drive/build-your-own-image-classifier/data/pets&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: Make sure to terminate your Colab session once you’ve completed this task &lt;em&gt;(in order to avoid using up unneccesary resources)&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;cleaning-your-image-dataset&quot;&gt;Cleaning your Image Dataset&lt;/h2&gt;

&lt;p&gt;Your next step is to clean that dataset.&lt;/p&gt;

&lt;p&gt;I’ve created another notebook do this (this makes it easy to keep the notebook short and easy to understand).&lt;/p&gt;

&lt;p&gt;So click again on the notebook below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/butchland/build-your-own-image-classifier/blob/master/colab-clean-image-dataset.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After running the notebook above, you should now have a cleaned up image dataset stored on your Google Drive.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you didn’t modify the default project name, the file &lt;code class=&quot;highlighter-rouge&quot;&gt;cleaned_pets.tgz&lt;/code&gt; should be in your Google Drive under the folder &lt;code class=&quot;highlighter-rouge&quot;&gt;/My Drive/build-your-own-image-classifier/data/pets&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;building-a-neural-network-image-classifier&quot;&gt;Building a Neural Network Image Classifier&lt;/h2&gt;

&lt;p&gt;You’re now ready to build a Neural Network Image Classifier.&lt;/p&gt;

&lt;p&gt;This step is also known as training a neural network model.&lt;/p&gt;

&lt;p&gt;In addition to training the model, we also need to export it to what is known as a “pickle” format (into the file named &lt;code class=&quot;highlighter-rouge&quot;&gt;export.pkl&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Again, click on the button below and follow the steps outlined in the notebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/butchland/build-your-own-image-classifier/blob/master/colab-build-image-classifier.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At the end of running this notebook, you should have an exported classifier(&lt;code class=&quot;highlighter-rouge&quot;&gt;export.pkl&lt;/code&gt;) in your Google drive.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you didn’t modify the default project name, the file &lt;code class=&quot;highlighter-rouge&quot;&gt;export.pkl&lt;/code&gt; should be in your Google Drive under the folder &lt;code class=&quot;highlighter-rouge&quot;&gt;/My Drive/build-your-own-image-classifier/models/pets&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;test-your-image-classifier-app-on-colab&quot;&gt;Test your Image Classifier App on Colab&lt;/h2&gt;

&lt;p&gt;We can now test our image classifier app on Colab.&lt;/p&gt;

&lt;p&gt;Again, click on the button below to test the application on Colab.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/butchland/build-your-own-image-classifier/blob/master/colab-test-image-classifier.ipynb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once your image classifier app has been tested, you have accomplished your goal – to build an app that runs an image classifier.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But wouldn’t it be great if you could make it easy for others to use your app?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After all, you are now ready to share it to the world (or at the very least, your friends, family and colleagues…)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(&lt;a href=&quot;/butchland-machine-learning-notes/machine%20learning/2020/10/05/byoic-on-colab-part2.html&quot; target=&quot;_blank&quot;&gt;TO BE CONTINUED IN PART 2&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you liked this article, and most specially if you were able to build your own image classifier based on this article, give me a shoutout on &lt;a href=&quot;https://twitter.com/butchland&quot; target=&quot;_blank&quot;&gt;my twitter account @butchland&lt;/a&gt; or message me on the &lt;a href=&quot;https://forums.fast.ai/u/butchland&quot; target=&quot;_blank&quot;&gt;fastai forums: @butchland&lt;/a&gt;, I’d really appreciate it!&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;The software used to clean your data, build your classifier and run your application is largely based on the &lt;a href=&quot;https://docs.fast.ai&quot; target=&quot;_blank&quot;&gt;fastai&lt;/a&gt; python package, written by Jeremy Howard and Sylvain Gugger.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://jupyter.org/&quot; target=&quot;_blank&quot;&gt;Jupyter Notebook&lt;/a&gt; environment that enables us to run interactive documents to build our image classifier won the 2017 ACM Software System Award and is widely used by scientists, researchers and students around the world.&lt;/p&gt;

&lt;p&gt;The software package used to collect the images (&lt;a href=&quot;https://joedockrill.github.io/jmd_imagescraper/&quot; target=&quot;_blank&quot;&gt;jmd_imagescraper&lt;/a&gt;) was built by Joe Dockrill (&lt;a href=&quot;https://forums.fast.ai/u/joedockrill&quot; target=&quot;_blank&quot;&gt;@joedockrill&lt;/a&gt;), one of the students of the fast.ai course.&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://butchland.github.io/butchland-machine-learning-notes/images/image-classifier-logo.png" /><media:content medium="image" url="https://butchland.github.io/butchland-machine-learning-notes/images/image-classifier-logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">fast.ai Deep Learning Adventure Treks (Part 2)</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/07/06/fastai-deeplearning-adventure-guides-part2.html" rel="alternate" type="text/html" title="fast.ai Deep Learning Adventure Treks (Part 2)" /><published>2020-07-06T00:00:00-05:00</published><updated>2020-07-06T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/07/06/fastai-deeplearning-adventure-guides-part2</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/07/06/fastai-deeplearning-adventure-guides-part2.html">&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/images/dora-map-explorer.png&quot; alt=&quot;dora explorer map&quot; title=&quot;Dora the Explorer Map - credits to https://www.mobygames.com/game/windows/dora-the-explorer-swipers-big-adventure/screenshots&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;fastai-deep-learning-adventure-treks-part-2&quot;&gt;fast.ai Deep Learning Adventure Treks (Part 2)&lt;/h1&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;fast.ai Deep Learning Adventure Treks&lt;/strong&gt; is an initiative to provide first-time fast.ai learners with a more formalized, structured study group, with a fixed schedule, led by more experienced fast.ai community members on a volunteer basis, to make the online course (MOOC version) more engaging and effective specially for non-programmers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/butchland-machine-learning-notes/fastai/2020/06/28/fastai-deeplearning-adventure-guides.html&quot;&gt;Part 1&lt;/a&gt; covers the rationale. Part 2  covers the program’s vision and use cases.&lt;/p&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;

&lt;p&gt;In order to give a better picture of how the program can be implemented, we created user personas to provide a more personal, relatable picture of the program’s use cases.&lt;/p&gt;

&lt;h3 id=&quot;user-personas&quot;&gt;User Personas&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hue&lt;/strong&gt; is a dermatologist from Vietnam who wants to develop telemedicine diagnostic mobile apps.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nadja&lt;/strong&gt; is an agriculturist who is working with rice farmers from Bangladesh. She is hoping to combine satellite and drone images with agricultural field reports and weather forecasts to improve crop yield estimates.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sonia&lt;/strong&gt; is a health informatics expert who is hoping to use data from her country’s health insurance filings improve public health policy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nikolai&lt;/strong&gt; is a journalist studying his country’s social media growth patterns and how it has influenced governmental policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They are all hoping to add deep learning to their toolkits and want to learn how to apply it to their domains.&lt;/p&gt;

&lt;h3 id=&quot;pre-implementation-scenarios&quot;&gt;Pre-implementation Scenarios&lt;/h3&gt;

&lt;h4 id=&quot;hues-setup-issues&quot;&gt;Hue’s setup issues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Hue has just learned about the fast.ai MOOC from her twitter feed and is interested in taking the course. She doesn’t have a programming background and is a little intimidated by the topic.
She is however very determined and is hopeful after watching the first lecture that she can apply deep learning to help her in diagnosing skin diseases.&lt;/li&gt;
  &lt;li&gt;She plans to work with some mobile devs who may not have the deep learning skillsets, but she plans to develop the diagnostic deep learning models herself (since she knows how to diagnose dermatological problems from images) and serve the models via an API.&lt;/li&gt;
  &lt;li&gt;After having some problems setting up her jupyter notebook environment, she joined the forums and asked for help. While there were some responses to her questions, it took her several days to resolve them, primarily due to timezone differences.&lt;/li&gt;
  &lt;li&gt;She also felt a little lost with some of the advice, as in most of them assumed a deeper computer technical background than what she had.&lt;/li&gt;
  &lt;li&gt;She felt she could have resolved her issues a lot quicker, had their been some hands-on assistance.&lt;/li&gt;
  &lt;li&gt;She is also looking for assistance in learning Python so she can better handle the course material.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;nadjas-customized-workflow-applications&quot;&gt;Nadja’s customized workflow applications&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Nadja also heard about the fast.ai MOOC from a colleague. Although her work already involves analyzing satellite images and weather forecasts separately, she’s interested in a customized workflow that integrates all those data sources into deep learning models so they can scale up personalized predictions and insights for the farmer’s crop yields.&lt;/li&gt;
  &lt;li&gt;Although her office employs data scientists, none of them have a lot of experience with deep learning and she’s reluctant to bother them for assistance.&lt;/li&gt;
  &lt;li&gt;She uses the forums as much as she can, but wishes for a more person-to-person interaction even if only an online one, since her goal of combining images with weather forecasts is not a common thing being done in her field.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sonias-schedule&quot;&gt;Sonia’s schedule&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Sonia is hoping to add deep learning to her data science skills. She has been using Python in her other projects, and is interested in building a recommendation system for local goverment units based on public health data from health insurance claims of their constituents to hopefully improve public health outcomes.&lt;/li&gt;
  &lt;li&gt;She started watching the videos and participating in the forums. She is eager to learn new skills, but also realizes her intended project is quite ambitious and instead is working on smaller, simpler projects.&lt;/li&gt;
  &lt;li&gt;Her problem is sustaining her interest and momentum. While she welcomes the fact that the MOOC allows her to learn on her own pace, she also knows that  a regular schedule with defined weekly objectives can help her maintain a consistent pace to her advancement in deep learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;nikolais-networking-goals&quot;&gt;Nikolai’s networking goals&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Nikolai is a data journalist and is familiar with Tableau and a smattering of R, but not python. He intends to learn deep learning especially NLP techniques, in order to apply them to preprocessing text from social media sources to be used for analysis later.&lt;/li&gt;
  &lt;li&gt;Nikolai has been watching the videos and participating in the forums, but is also hoping for more social interaction with the community members.&lt;/li&gt;
  &lt;li&gt;While he’s trying to gain as much understanding of deep learning as he can, he is also hoping he can build connections from his country’s local fast.ai community and leverage their diverse talents to focus on problems he’s interested in.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;a-vision-for-the-fastai-deep-learning-adventure-treks&quot;&gt;A vision for the fast.ai Deep Learning Adventure Treks&lt;/h3&gt;

&lt;p&gt;The goal of the fast.ai Deep Learning Adventure Treks program is to help learners of the fast.ai MOOC become more effective. A following idealized scenario gives us a glimpse of how the initiative can address the issues discussed above.&lt;/p&gt;

&lt;h4 id=&quot;a-deep-learning-expedition&quot;&gt;A Deep learning expedition&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Hue finds out about the fast.ai Deep Learning Adventure Treks. She looks for and finds a program starting in about a week with a schedule geared for her timezone.&lt;/li&gt;
  &lt;li&gt;She signs up and attends an online orientation via a Zoom call. This particular Adventure Trek program is run by Zach, a long time fast.ai volunteer. She’s also happy to find that Nadja, a fellow school alumni, has also signed up for the same group.&lt;/li&gt;
  &lt;li&gt;As part of the program requirements, they sign a fast.ai Deep Learning Adventure Trekker’s agreement that asks for their commitment to the community’s values of inclusion and respect as well as to exert their best efforts to completing the program. They also commit to helping out other participants and contribute to the community in the future.&lt;/li&gt;
  &lt;li&gt;The orientation also included introductions as well as some online social games to break the ice among the group members.&lt;/li&gt;
  &lt;li&gt;She and her fellow expedition members (all 7 of them) are given a proposed schedule by Zach, their guide. They discuss it and modify it based on their personal schedules (A sample schedule is provided below).&lt;/li&gt;
  &lt;li&gt;Based on her schedule, she requested if she could skip the group video watching activities, but promised to join the discussion after (she planned to watch the videos on her daily commute instead)&lt;/li&gt;
  &lt;li&gt;As part of her mini-projects, she’s signed up to pair with Nikolai, who seems interested in NLP, to recreate a twitter sentiment analysis notebook. While she doesn’t plan on using NLP in her project, she’s also keen to round out her knowledge of deep learning to include areas other than where she plans to specialize in.&lt;/li&gt;
  &lt;li&gt;She’s also thinking of teaming up with her fellow group member Sonia as well as her friend Nadja for their final group project, since Sonia has some programming skills, which she and Nadja did not. Besides, they seemed to have bonded quite well during the orientation.&lt;/li&gt;
  &lt;li&gt;She’s also thinking of turning her proposed team into an accountability group, a recommendation from their Adventure Trek guide. By making herself accountable to an external group, she hopes to become more consistent in completing the study group activities and make her deep learning study habits stick.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sample-program-schedule&quot;&gt;Sample Program Schedule&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Base Camp (Orientation)&lt;/li&gt;
  &lt;li&gt;Station 1 (Intro)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 1 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion
        &lt;ul&gt;
          &lt;li&gt;workshop activity: setup jupyter notebook environment&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 1) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 1 + troubleshooting session&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 2 (Production)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 2 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion
        &lt;ul&gt;
          &lt;li&gt;workshop activity: setup Azure search keys + run binder&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 2) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 2 + troubleshooting session&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 3 (MNIST)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 3 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion&lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 4) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 4 + troubleshooting session
        &lt;ul&gt;
          &lt;li&gt;workshop activity: build a nn model from scratch&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 4 (Pets)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 4 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion
        &lt;ul&gt;
          &lt;li&gt;workshop activity: exercises using Datablocks&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 5) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 5 + troubleshooting session&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 5 (Ethics)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 5 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion&lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 3) + Questionnaire roundtable discussion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 6 (Multicat)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 6 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion
        &lt;ul&gt;
          &lt;li&gt;workshop activity: redo pets and mnist using multicat&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 6) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 6 + troubleshooting session&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 7 (Collab and Tabular)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 7 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion&lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 8) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 8 + troubleshooting session&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;book reading activity (Chapter 9) + Questionnaire roundtable discussion&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;run notebook 9 + troubleshooting session&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Station 8 (NLP)
    &lt;ul&gt;
      &lt;li&gt;video watch Lecture 8 + Q&amp;amp;A discussion after&lt;/li&gt;
      &lt;li&gt;study group discussion&lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 10) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 10  + troubleshooting session&lt;/li&gt;
      &lt;li&gt;book reading activity (Chapter 12) + Questionnaire roundtable discussion&lt;/li&gt;
      &lt;li&gt;run notebook 12  + troubleshooting session&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Summit (Group Project)
    &lt;ul&gt;
      &lt;li&gt;group presentations&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sample-schedules&quot;&gt;Sample Schedules&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;video lecture watching - Tuesdays 6-8pm every 2 or 3  weeks&lt;/li&gt;
  &lt;li&gt;video lecture Q&amp;amp;A discussion - 8-9pm Tuesdays following video watching&lt;/li&gt;
  &lt;li&gt;video lecture study group Q&amp;amp;A - Thursdays 6-8pm every 2 weeks&lt;/li&gt;
  &lt;li&gt;book reading chapter - Tuesdays 6-9pm  after video lecture watching&lt;/li&gt;
  &lt;li&gt;book chapter questionnaire Q&amp;amp;A - following book reading&lt;/li&gt;
  &lt;li&gt;run notebooks and troubleshooting - Thursdays 6-8 after book reading&lt;/li&gt;
  &lt;li&gt;group projects - presentations scheduled 2 weeks after last book reading.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-activities&quot;&gt;Additional Activities&lt;/h3&gt;

&lt;p&gt;In addition to programmed study groups centered on the fast.ai lectures,
additional related topics may also be covered (depending on the students’ learning needs), such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Python programming&lt;/li&gt;
  &lt;li&gt;git&lt;/li&gt;
  &lt;li&gt;pandas&lt;/li&gt;
  &lt;li&gt;scikit-learn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, participants can also undertake individual mini-projects. The Adventure Trek guides also curate and provide pointers to new members on what mini-projects they can undertake to level up their deep learning skills given their current skill level.&lt;/p&gt;

&lt;p&gt;As envisioned, these activities can help fast.ai MOOC course increase the number of learners who go on to finish the course and more effective in achieving its goals.&lt;/p&gt;

&lt;h3 id=&quot;motivations-for-the-fastai-deep-learning-adventure-trek-guides&quot;&gt;Motivations for the fast.ai Deep Learning Adventure Trek Guides&lt;/h3&gt;

&lt;p&gt;So aside from altruistic motives, why would an fast.ai Deep Learning Adventure Trek Guide volunteer?&lt;/p&gt;

&lt;p&gt;The primary reason, I believe, is because of the truism that the best way to learn a topic is by teaching it. It will help the guide gain a deeper understanding of deep learning, as well as cementing the foundations into their knowledge base.&lt;/p&gt;

&lt;p&gt;Another reason can be that providing social interactions in a supportive environment is a reward in itself.&lt;/p&gt;

&lt;p&gt;Lastly, the ability to teach Deep Learning is a valuable skill in of itself, and may lead to  opportunities later.&lt;/p&gt;

&lt;p&gt;Leading an adventure trek provides valuable experience to develop these skills, as well as exposing the guide to a diverse set of domains where deep learning can be applicable.&lt;/p&gt;

&lt;p&gt;Further incentives (such as certification, reviews, or financial remuneration) maybe added in the future, based on feedback.&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><summary type="html">fast.ai Deep Learning Adventure Treks (Part 2)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://butchland.github.io/butchland-machine-learning-notes/images/dora-map-explorer.png" /><media:content medium="image" url="https://butchland.github.io/butchland-machine-learning-notes/images/dora-map-explorer.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">fast.ai Deep Learning Adventure Treks (Part 1)</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/28/fastai-deeplearning-adventure-guides.html" rel="alternate" type="text/html" title="fast.ai Deep Learning Adventure Treks (Part 1)" /><published>2020-06-28T00:00:00-05:00</published><updated>2020-06-28T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/28/fastai-deeplearning-adventure-guides</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/28/fastai-deeplearning-adventure-guides.html">&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/images/dora-map-explorer.png&quot; alt=&quot;dora explorer map&quot; title=&quot;Dora the Explorer Map - credits to https://www.mobygames.com/game/windows/dora-the-explorer-swipers-big-adventure/screenshots&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;fastai-deep-learning-adventure-treks-part-1---rationale&quot;&gt;fast.ai Deep Learning Adventure Treks (Part 1 - Rationale)&lt;/h1&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;fast.ai Deep Learning Adventure Treks&lt;/strong&gt; is an initiative to provide first-time fast.ai learners with a more formalized, structured study group, with a fixed schedule, led by more experienced fast.ai community members on a volunteer basis, to make the online course (MOOC version) more engaging and effective specially for non-programmers.&lt;/p&gt;

&lt;h2 id=&quot;rationale&quot;&gt;Rationale&lt;/h2&gt;

&lt;h3 id=&quot;my-fastai-experience&quot;&gt;My fast.ai experience&lt;/h3&gt;

&lt;p&gt;When I first joined the fast.ai MOOC (Practical Deep Learning for Coders) around January last year, I first felt very enthusiastic and energized to view the lectures, run the notebooks, and participate in the forums – as a result, I’ve learned a lot from the first 4 lectures.&lt;/p&gt;

&lt;p&gt;But as the weeks passed, my interest waned, not because I wasn’t learning anything – quite the opposite – I learned more from the fast.ai MOOC about applying Deep Learning in the real world than any other ML course I had previously taken.&lt;/p&gt;

&lt;p&gt;I still listened to the last 3 lectures, I but didn’t do the additional coursework that would have cemented the concepts into my mind.&lt;/p&gt;

&lt;p&gt;By the time the 2nd part of the course came, I had an even harder time catching up, and thus ended my interest in the latter parts of the Part 2 of the course.&lt;/p&gt;

&lt;p&gt;A major factor for this waning interest is that learning remotely can be quite isolating – and despite having a fantastic and supportive community in the forums, I felt at a loss in figuring out what personal projects I could pursue to further deepen my understanding of the topics.&lt;/p&gt;

&lt;p&gt;Though I tried to maintain a weekly schedule (following the live course’s pace), there was nothing that really pushed me to complete something each week.&lt;/p&gt;

&lt;p&gt;I suspect that I am not alone in this – there may have been a lot of MOOC learners whose interest would not have waned had there been more support for their learning needs. &lt;em&gt;I know a learner’s individual &lt;strong&gt;tenacity&lt;/strong&gt; is key to success in this course, but anything we can do to make it more effective is worth pursuing, I think.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So when I got invited to the 2020 live online edition of the fastai course, I was stoked and energized again to deepen my understanding of machine learning and deep learning.&lt;/p&gt;

&lt;p&gt;This time around, I resolved to make my learning more consistent and effective.&lt;/p&gt;

&lt;p&gt;I think I did a lot better this time around – I was able to finish the 8 lectures and I was more active in the forums and did the course work assignments more religiously than last time.&lt;/p&gt;

&lt;p&gt;Moreover, as of today, almost two months after the live online course officially ended, I have kept up my interest and am still doing everything I can everyday to deepen my understanding of the field (Of course, another factor that may have inadvertently helped was the quarantine due to COVID-19).&lt;/p&gt;

&lt;p&gt;However, I believe the biggest factor in my sustained interest this time around was due to the fact that I joined an online study group during the live online weekly lectures, – actually 2 study groups: the Unofficial SF Study Group (despite the fact that I was based in a timezone 16 hours ahead), and the beginners study group led by Wayde Gilliam (@wgpubs) every Thursday.&lt;/p&gt;

&lt;p&gt;More importantly, the Unofficial Study Group resolved to continue meeting even after the end of the live lectures last May 6 to further encourage its members to keep pursuing their learning.&lt;/p&gt;

&lt;p&gt;It has since evolved into 3 related activities:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A biweekly meeting for presenting projects such as kaggle competitions, and blog posts or software projects&lt;/li&gt;
  &lt;li&gt;A weekly book rereading where participants set aside a 3 hour block of time to read a fastbook chapter followed by 30 minute discussion&lt;/li&gt;
  &lt;li&gt;An accountability mini-group where we meet weekly to discuss our weekly learning goals and encourage accountability for those goals.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h3&gt;

&lt;p&gt;For me, these activities have not only been effective in keeping my interest in the course (aside from participating in the forum discussions) but have also helped relieve the isolation brought on by the COVID-19 pandemic quarantine.&lt;/p&gt;

&lt;p&gt;Aside from these, the fast.ai community has (especially during the live lectures) also provided additional activities – I would highlight the videos and meetings on the fastai source code review as well as other videos by Zach Mueller.&lt;/p&gt;

&lt;p&gt;These resources have all helped enrich my learning experience, but I think the social experience given by an online study group was the most effective addition to make my fast.ai learning experience more effective.&lt;/p&gt;

&lt;h3 id=&quot;suggestions-for-improvement&quot;&gt;Suggestions for Improvement&lt;/h3&gt;

&lt;p&gt;Having had a long experience with conducting trainings on software development, I have always thought about how to make the online fast.ai learning experience more effective not only for myself but for others like me.&lt;/p&gt;

&lt;p&gt;I totally support fast.ai’s goal of democratizing AI and Deep Learning and making it more accessible.&lt;/p&gt;

&lt;p&gt;An important part of this effort is making it more accessible outside of the US, in less developed countries like Asia and Africa (which is why making the lecture transcripts and captions available in other languages is also important).&lt;/p&gt;

&lt;p&gt;Also important is making the community more diverse and inclusive, especially in encouraging people from different backgrounds to join the community, making them feel welcome and helping them achieve success in learning deep learning so they can apply it to their domains and societies.&lt;/p&gt;

&lt;p&gt;I think one area where the fast.ai support is especially lacking is helping those learners who may not have a programming background.&lt;/p&gt;

&lt;p&gt;While the fast.ai’s “whole game” approach and goals encourages people who may not have a strong programming background to learn deep learning and apply it to their own fields, I think we can provide better ways to support them and get over the initial hump of learning the Python basics (plus other software development related stuff like git, using the bash command line, etc.).&lt;/p&gt;

&lt;p&gt;I think this is quite possible for the fast.ai community to achieve – given that the majority of its members, I believe, do have programming chops to teach the other learners who might be having difficulty in that area.&lt;/p&gt;

&lt;p&gt;For these reasons, and based on my experiences, I have come to propose the &lt;strong&gt;fast.ai Deep Learning Adventure Treks&lt;/strong&gt; program as an initiative to help make the online fast.ai learning experience more effective.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;continued on to&lt;/em&gt; &lt;strong&gt;&lt;a href=&quot;/butchland-machine-learning-notes/fastai/2020/07/06/fastai-deeplearning-adventure-guides-part2.html&quot;&gt;Part 2&lt;/a&gt;&lt;/strong&gt; which covers the program’s vision and use cases.&lt;/p&gt;</content><author><name></name></author><summary type="html">fast.ai Deep Learning Adventure Treks (Part 1 - Rationale)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://butchland.github.io/butchland-machine-learning-notes/images/dora-map-explorer.png" /><media:content medium="image" url="https://butchland.github.io/butchland-machine-learning-notes/images/dora-map-explorer.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fastbook Chapter 8 Questionnaire Answers</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/24/fast-ai-chapter-8-collab-filter-deep-dive-questionnaire-answers.html" rel="alternate" type="text/html" title="Fastbook Chapter 8 Questionnaire Answers" /><published>2020-06-24T00:00:00-05:00</published><updated>2020-06-24T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/24/fast-ai-chapter-8-collab-filter-deep-dive-questionnaire-answers</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/24/fast-ai-chapter-8-collab-filter-deep-dive-questionnaire-answers.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-24-fast-ai-chapter-8-collab-filter-deep-dive-questionnaire-answers.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Questionnaire&quot;&gt;Questionnaire&lt;a class=&quot;anchor-link&quot; href=&quot;#Questionnaire&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;What problem does collaborative filtering solve?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How does it solve it?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Why might a collaborative filtering predictive model fail to be a very useful recommendation system?&lt;/li&gt;
&lt;li&gt;What does a crosstab representation of collaborative filtering data look like?&lt;/li&gt;
&lt;li&gt;Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!).&lt;/li&gt;
&lt;li&gt;What is a latent factor? Why is it &quot;latent&quot;?&lt;/li&gt;
&lt;li&gt;What is a dot product? Calculate a dot product manually using pure Python with lists.&lt;/li&gt;
&lt;li&gt;What does &lt;code&gt;pandas.DataFrame.merge&lt;/code&gt; do?&lt;/li&gt;
&lt;li&gt;What is an embedding matrix?&lt;/li&gt;
&lt;li&gt;What is the relationship between an embedding and a matrix of one-hot-encoded vectors?&lt;/li&gt;
&lt;li&gt;Why do we need &lt;code&gt;Embedding&lt;/code&gt; if we could use one-hot-encoded vectors for the same thing?&lt;/li&gt;
&lt;li&gt;What does an embedding contain before we start training (assuming we're not using a pretained model)?&lt;/li&gt;
&lt;li&gt;Create a class (without peeking, if possible!) and use it.&lt;/li&gt;
&lt;li&gt;What does &lt;code&gt;x[:,0]&lt;/code&gt; return?&lt;/li&gt;
&lt;li&gt;Rewrite the &lt;code&gt;DotProduct&lt;/code&gt; class (without peeking, if possible!) and train a model with it.&lt;/li&gt;
&lt;li&gt;What is a good loss function to use for MovieLens? Why? &lt;/li&gt;
&lt;li&gt;What would happen if we used cross-entropy loss with MovieLens? How would we need to change the model?&lt;/li&gt;
&lt;li&gt;What is the use of bias in a dot product model?&lt;/li&gt;
&lt;li&gt;What is another name for weight decay?&lt;/li&gt;
&lt;li&gt;Write the equation for weight decay (without peeking!).&lt;/li&gt;
&lt;li&gt;Write the equation for the gradient of weight decay. Why does it help reduce weights?&lt;/li&gt;
&lt;li&gt;Why does reducing weights lead to better generalization?&lt;/li&gt;
&lt;li&gt;What does &lt;code&gt;argsort&lt;/code&gt; do in PyTorch?&lt;/li&gt;
&lt;li&gt;Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why/why not?&lt;/li&gt;
&lt;li&gt;How do you print the names and details of the layers in a model?&lt;/li&gt;
&lt;li&gt;What is the &quot;bootstrapping problem&quot; in collaborative filtering?&lt;/li&gt;
&lt;li&gt;How could you deal with the bootstrapping problem for new users? For new movies?&lt;/li&gt;
&lt;li&gt;How can feedback loops impact collaborative filtering systems?&lt;/li&gt;
&lt;li&gt;When using a neural network in collaborative filtering, why can we have different numbers of factors for movies and users?&lt;/li&gt;
&lt;li&gt;Why is there an &lt;code&gt;nn.Sequential&lt;/code&gt; in the &lt;code&gt;CollabNN&lt;/code&gt; model?&lt;/li&gt;
&lt;li&gt;What kind of model should we use if we want to add metadata about users and items, or information such as date and time, to a collaborative filtering model?&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Further-Research&quot;&gt;Further Research&lt;a class=&quot;anchor-link&quot; href=&quot;#Further-Research&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Take a look at all the differences between the &lt;code&gt;Embedding&lt;/code&gt; version of &lt;code&gt;DotProductBias&lt;/code&gt; and the &lt;code&gt;create_params&lt;/code&gt; version, and try to understand why each of those changes is required. If you're not sure, try reverting each change to see what happens. (NB: even the type of brackets used in &lt;code&gt;forward&lt;/code&gt; has changed!)&lt;/li&gt;
&lt;li&gt;Find three other areas where collaborative filtering is being used, and find out what the pros and cons of this approach are in those areas.&lt;/li&gt;
&lt;li&gt;Complete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book's website and the fast.ai forum for ideas. Note that there are more columns in the full dataset—see if you can use those too (the next chapter might give you ideas).&lt;/li&gt;
&lt;li&gt;Create a model for MovieLens that works with cross-entropy loss, and compare it to the model in this chapter.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Fastbook Chapter 2 Production</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/23/fastbook-chapter-2-production.html" rel="alternate" type="text/html" title="Fastbook Chapter 2 Production" /><published>2020-06-23T00:00:00-05:00</published><updated>2020-06-23T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/23/fastbook-chapter-2-production</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/23/fastbook-chapter-2-production.html">&lt;p&gt;&lt;em&gt;These are my notes from my reading of the fastai book&lt;/em&gt;
&lt;em&gt;by Jeremy Howard and Sylvain Gugger&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;from-model-to-production&quot;&gt;From Model to Production&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Chapter Objective&lt;/strong&gt;: Learn the end-to-end process of building DL application
    &lt;ul&gt;
      &lt;li&gt;collect data&lt;/li&gt;
      &lt;li&gt;build model&lt;/li&gt;
      &lt;li&gt;deploy app&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-practice-of-deep-learning&quot;&gt;The Practice of Deep Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DL can solve a lot of challenging problems that were previously hard to solve.
    &lt;ul&gt;
      &lt;li&gt;As a DL beginner - find sweet spot of problems
        &lt;ul&gt;
          &lt;li&gt;similar to example problems&lt;/li&gt;
          &lt;li&gt;get extremely useful results&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;but DL is not magic!
        &lt;ul&gt;
          &lt;li&gt;same lines of code wont work for all problems&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To learn DL quickly
    &lt;ul&gt;
      &lt;li&gt;find sweet spot of challenging problems
        &lt;ul&gt;
          &lt;li&gt;quick to get extremely useful results&lt;/li&gt;
          &lt;li&gt;but difficult enough to always learn something new&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Understanding capabilities and constraints of DL
    &lt;ul&gt;
      &lt;li&gt;underestimating constraints and overestimating capabilities of DL
        &lt;ul&gt;
          &lt;li&gt;can lead to poor results, consequently frustration&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;overestimating constraints and underestimating capabilities
        &lt;ul&gt;
          &lt;li&gt;can lead to reticence in applying DL, even when it can be reachable&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;underestimating capabilities can lead to reticence in applying DL&lt;/li&gt;
          &lt;li&gt;underestimating constraints can lead to failure to consider and react to issues arising from using DL&lt;/li&gt;
          &lt;li&gt;best to keep an open mind
            &lt;ul&gt;
              &lt;li&gt;DL might solve part of your problem with less data or complexity&lt;/li&gt;
              &lt;li&gt;Design a process to find specific capabilities and constraints related to your particular area&lt;/li&gt;
              &lt;li&gt;No risky bets - gradually roll out models so they don’t create significant risks
                &lt;ul&gt;
                  &lt;li&gt;backtest prior to production&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;starting-your-project&quot;&gt;Starting your project&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure that you have a project to work on
    &lt;ul&gt;
      &lt;li&gt;only by working through projects will gain experience building and using models&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Considerations in looking for your project
    &lt;ul&gt;
      &lt;li&gt;most important consideration: data availability&lt;/li&gt;
      &lt;li&gt;get started quickly&lt;/li&gt;
      &lt;li&gt;iterate quickly - goal is not to start with “perfect” dataset, but start and iterate quickly&lt;/li&gt;
      &lt;li&gt;iterate end-to-end
        &lt;ul&gt;
          &lt;li&gt;dont spend too much time polishing each step&lt;/li&gt;
          &lt;li&gt;complete every step in reasonable time&lt;/li&gt;
          &lt;li&gt;do it all the way to the end and then come back to iterate again and again&lt;/li&gt;
          &lt;li&gt;evaluate process to find areas to focus on that make the biggest gains in end result&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;work through the book
        &lt;ul&gt;
          &lt;li&gt;complete lots of small experiments
            &lt;ul&gt;
              &lt;li&gt;by running and adjusting the provided notebooks&lt;/li&gt;
              &lt;li&gt;at the same time, gradually develop your own projects&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;get experience with all tools and techniques discussed in the book
            &lt;ul&gt;
              &lt;li&gt;learn by practicing and failing&lt;/li&gt;
              &lt;li&gt;DL is still artisanal practice
                &lt;ul&gt;
                  &lt;li&gt;nothing beats applied practical experience in training models&lt;/li&gt;
                  &lt;li&gt;backed by theoretical understanding of DL principles&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;show effective results
            &lt;ul&gt;
              &lt;li&gt;nothing beats a working prototype&lt;/li&gt;
              &lt;li&gt;get organization buy-in for DL projects&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Easiest to get started on projects
            &lt;ul&gt;
              &lt;li&gt;where data is already available (e.g. public datasets)&lt;/li&gt;
              &lt;li&gt;related to something you are already doing (where you already have access to data)&lt;/li&gt;
              &lt;li&gt;be creative if data is not available
                &lt;ul&gt;
                  &lt;li&gt;find related data in a similar domain or tackling a slightly different problem&lt;/li&gt;
                  &lt;li&gt;working on related domain will still help you identify shortcuts or workarounds&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;When starting DL, Not a good idea to branch out to different areas
            &lt;ul&gt;
              &lt;li&gt;esp. areas where DL has not been applied before&lt;/li&gt;
              &lt;li&gt;if you have problem - you wont know whether
                &lt;ul&gt;
                  &lt;li&gt;you simply made a mistake or&lt;/li&gt;
                  &lt;li&gt;DL is not a good fit for the application&lt;/li&gt;
                  &lt;li&gt;hard to look for help&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Start with a problem that someone else had good results
            &lt;ul&gt;
              &lt;li&gt;similar to what you are trying to achieve&lt;/li&gt;
              &lt;li&gt;or you can convert data to some similar format&lt;/li&gt;
              &lt;li&gt;look at areas where DL has been successful&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-state-of-deep-learning&quot;&gt;The State of Deep Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Consider whether DL is a good fit to your application by looking where DL capabilities are mature.
    &lt;ul&gt;
      &lt;li&gt;but DL is evolving quickly, so previous historical constraints may have been overcome by latest research&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;computer-vision&quot;&gt;Computer Vision&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Areas where DL is mature:
    &lt;ul&gt;
      &lt;li&gt;object recognition - what items are in an image (e.g.  everyday objects, faces, medical images - xrays, MRI, - cancerous lesions, etc)&lt;/li&gt;
      &lt;li&gt;object detection - where the objects are in an image
        &lt;ul&gt;
          &lt;li&gt;variant: segmentation - what object each pixel belongs to&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DL image recognition may not be good
    &lt;ul&gt;
      &lt;li&gt;recognizing images that are significantly different from ones used in training
        &lt;ul&gt;
          &lt;li&gt;eg. if no black/white images in training data, model might not be good when used on b/w images.&lt;/li&gt;
          &lt;li&gt;also not good for hand-drawn images if they are not part of training data.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;no general way to check for missing types of images in training data
        &lt;ul&gt;
          &lt;li&gt;but there are ways to check if unexpected image types are encountered in production
            &lt;ul&gt;
              &lt;li&gt;AKA checking for &lt;em&gt;out-of-domain data&lt;/em&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Labeling image data can be slow and expensive
        &lt;ul&gt;
          &lt;li&gt;currently requires manual effort&lt;/li&gt;
          &lt;li&gt;lot of effort to make this better
            &lt;ul&gt;
              &lt;li&gt;tools to make labelling faster and easier&lt;/li&gt;
              &lt;li&gt;require handcrafted labels to train accurate object detection models
                &lt;ul&gt;
                  &lt;li&gt;one useful technique: &lt;em&gt;data augmentation&lt;/em&gt;
                    &lt;ul&gt;
                      &lt;li&gt;synthetically generate variations of input data to improve capability of model&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Another possibility is to convert your data (which may not appear to be a computer vision problem) into images
        &lt;ul&gt;
          &lt;li&gt;images may then be amenable to computer vision algorithms for detection&lt;/li&gt;
          &lt;li&gt;example: convert audio to images of acoustic waveforms and use object detection to solve audio detection problem&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;text-nlp&quot;&gt;Text (NLP)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;DL in NLP is good at:
    &lt;ul&gt;
      &lt;li&gt;classifying short/long docs on
        &lt;ul&gt;
          &lt;li&gt;spam/not spam,&lt;/li&gt;
          &lt;li&gt;sentiment (positive/negative review)&lt;/li&gt;
          &lt;li&gt;author, source website, etc.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;generating context-appropriate text
        &lt;ul&gt;
          &lt;li&gt;but currently not good at generating &lt;em&gt;correct&lt;/em&gt; responses&lt;/li&gt;
          &lt;li&gt;can be dangerous, as text can be used for mass disinformation&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Other NLP applications
        &lt;ul&gt;
          &lt;li&gt;translate one language to another&lt;/li&gt;
          &lt;li&gt;summarize long documents&lt;/li&gt;
          &lt;li&gt;find mentions of a concept&lt;/li&gt;
          &lt;li&gt;but translations and summaries can contain incorrect information&lt;/li&gt;
          &lt;li&gt;actual usage: google translate and other online translation services - use DL&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;combining-text-and-images&quot;&gt;Combining Text and Images&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Combine text and images into single model
    &lt;ul&gt;
      &lt;li&gt;example DL app: generate captions for images
        &lt;ul&gt;
          &lt;li&gt;but no guarantee for accuracy&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;need to have human oversight due to possible errors
        &lt;ul&gt;
          &lt;li&gt;can be more productive/accurate than manual process with humans alone&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;tabular-data&quot;&gt;Tabular Data&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Analyzing time-series and tabular data, DL has made great strides&lt;/li&gt;
  &lt;li&gt;DL is generally used as part of ensemble of multiple types of models&lt;/li&gt;
  &lt;li&gt;DL is not a great improvement over RF (random forest) or GBM (gradient boosting methods)&lt;/li&gt;
  &lt;li&gt;but can handle columns with greater cardinality or columns with text (by using NLP)&lt;/li&gt;
  &lt;li&gt;Downside: DL takes longer to train than RF or GBM&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;recommendation-systems&quot;&gt;Recommendation Systems&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;very similar to tabular data, except they usually have a high-cardinality categorical variables represent users and products (or something similar - eg. movie review)&lt;/li&gt;
  &lt;li&gt;represents data as a sparse matrix (customers as rows and products as rows) representing purchases by a customer of a product&lt;/li&gt;
  &lt;li&gt;Apply collaborative filtering to fill in the matrix - this is used to make recommendations&lt;/li&gt;
  &lt;li&gt;Can combine other types of data -eg images, text, additional metadata such as customer information, previous transactions etc.&lt;/li&gt;
  &lt;li&gt;Can only predict, but not recommend - need to analyze how to turn predictions into recommendations&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;other-data-types&quot;&gt;Other Data Types&lt;/h4&gt;

&lt;p&gt;Other data types also exist&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Protein chains and genome sequences - can use NLP techniques&lt;/li&gt;
  &lt;li&gt;Audio recordings - convert to image as spectograms - can use CV techniques&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-drivetrain-approach&quot;&gt;The Drivetrain Approach&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Ensuring effective application of DL
    &lt;ul&gt;
      &lt;li&gt;DL can create accurate models but not useful&lt;/li&gt;
      &lt;li&gt;DL can also create inaccurate models that are useful&lt;/li&gt;
      &lt;li&gt;need to consider how your work will be used&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Drivetrain Approach - created by Jeremy, Margit Zwemer and Mike Loukides
    &lt;ul&gt;
      &lt;li&gt;discussed in book &lt;a href=&quot;https://www.oreilly.com/radar/drivetrain-approach-data-products/&quot;&gt;Designing Great Data Products&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;basic idea:
        &lt;ul&gt;
          &lt;li&gt;start considering objective&lt;/li&gt;
          &lt;li&gt;think about actions to take to meet objective&lt;/li&gt;
          &lt;li&gt;what data you have or can get to help decide on the action&lt;/li&gt;
          &lt;li&gt;build a model that can be used to determine best actions to get best results for your objective&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_2_Production_files/FastAI2020/Lesson_2_Production/Fastbook_Chapter_2_Production/drivetrain-approach.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Models are usually part of solution, they become part of the plumbing&lt;/li&gt;
  &lt;li&gt;Use data to produce actionable outcomes&lt;/li&gt;
  &lt;li&gt;Example: Google Search
    &lt;ul&gt;
      &lt;li&gt;define clear objective: what is user main objective in search? - find most relevant search result&lt;/li&gt;
      &lt;li&gt;consider levers for this objective: ranking of search results&lt;/li&gt;
      &lt;li&gt;consider data needed to produce ranking: implicit info based on links between pages provides rich data to determine which page is most relevant&lt;/li&gt;
      &lt;li&gt;after the first 3 steps, and determining what data is available and what data is needed, then build model to meet objective&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example: Recommendation Systems
    &lt;ul&gt;
      &lt;li&gt;objective: drive additional sales by recommending items which customers would never have purchased without the recommendation&lt;/li&gt;
      &lt;li&gt;lever: ranking of the recommendation&lt;/li&gt;
      &lt;li&gt;data needed: new data to generate recommendations to trigger new sales
        &lt;ul&gt;
          &lt;li&gt;require randomized experiments to collect data about wide range of recommendations for a wide range of customers (required to have info usable to optimize recommendations to increase sales)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;build two models - purchase probabilities conditional on seeing or not seeing the recommendation
        &lt;ul&gt;
          &lt;li&gt;the difference between the two models is a utility function for a given recommendation
            &lt;ul&gt;
              &lt;li&gt;low if algo recommends a familiar book already rejected by the customer&lt;/li&gt;
              &lt;li&gt;low if algo recommends a book  they would buy even without the recommendation&lt;/li&gt;
              &lt;li&gt;high if algo recommends a book they would not have bought without the recommendation&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;in practice, implementation of model requires more than just training the model.
        &lt;ul&gt;
          &lt;li&gt;also need to run experiments to collect more data&lt;/li&gt;
          &lt;li&gt;also need to consider how to incorporate models into the overall system&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gathering-data&quot;&gt;Gathering Data&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;How to find data for your project
    &lt;ul&gt;
      &lt;li&gt;For some projects, data needed might be online&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bear detector project
    &lt;ul&gt;
      &lt;li&gt;first project to build end-to-end model to app&lt;/li&gt;
      &lt;li&gt;distinguish between black, grizzly and teddy bear images&lt;/li&gt;
      &lt;li&gt;internet database of bear images is available&lt;/li&gt;
      &lt;li&gt;need to find and download bear images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Current Example: Bing Search Images
    &lt;ul&gt;
      &lt;li&gt;Signup at Microsoft - comes with &lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/&quot;&gt;Bing Image Search&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;see &lt;a href=&quot;https://github.com/fastai/course-v4/blob/master/nbs/02_production.ipynb&quot;&gt;course notebook&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;see &lt;a href=&quot;https://github.com/fastai/fastbook/blob/master/02_production.ipynb&quot;&gt;book chapter&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;API Key Page - after login - get API key &lt;a href=&quot;https://azure.microsoft.com/en-us/try/cognitive-services/my-apis/?api=search-api-v7&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;key = ‘xxx’&lt;/li&gt;
      &lt;li&gt;search and get image urls&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;results = search_images_bing(key, ‘grizzly bear’)
  ims = results.attrgot(‘content_url’)
  len(ims)
  dest = ‘images/grizzly.jpg’
  download_url(ims[0],dest)
  im = Image.open(dest)
  im.to_thumb(128,128)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_2_Production_files/FastAI2020/Lesson_2_Production/Fastbook_Chapter_2_Production/grizzly.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;download_images&lt;/code&gt; to download grizzly bears, black bears and teddy bears&lt;/li&gt;
  &lt;li&gt;download images to a different subfolder for each type&lt;/li&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;get_image_files&lt;/code&gt; to retrieve all images under common bears folder&lt;/li&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;verify_images&lt;/code&gt; to get corrupted images and &lt;code class=&quot;highlighter-rouge&quot;&gt;Path.unlink&lt;/code&gt; to delete them&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sidebar-getting-help-from-jupyter&quot;&gt;Sidebar: Getting Help from Jupyter&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;??&amp;lt;method_name&amp;gt;&lt;/code&gt; to show source&lt;/li&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;?&amp;lt;method_name&amp;gt;&lt;/code&gt; to show doc&lt;/li&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Tab&amp;gt;&lt;/code&gt; to get autocompletion&lt;/li&gt;
  &lt;li&gt;inside parenthesis, use &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Shift-Tab&amp;gt;&lt;/code&gt; to get function signature and short doc,
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Shift-Tab&amp;gt;&lt;/code&gt; twice to show more doc&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Shift-Tab&amp;gt;&lt;/code&gt; thrice to open a full window&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;doc(&amp;lt;func_name&amp;gt;)&lt;/code&gt; will open a window and links to source on Github plus &lt;a href=&quot;https://dev.fast.ai&quot;&gt;full doc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;%debug&lt;/code&gt; - open Python debugger which will let you inspect the contents of every variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;bing-image-search-results&quot;&gt;Bing Image Search Results&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Since models can only reflect data used to train them, and if the data used to train the models is *biased *then the model will also produce *biased results *
    &lt;ul&gt;
      &lt;li&gt;example : Bing Image Search for &lt;a href=&quot;https://www.bing.com/images/search?q=healthy+skin&amp;amp;scope=images&amp;amp;form=QBLH&amp;amp;sp=-1&amp;amp;pq=healthy+skin&amp;amp;sc=8-12&amp;amp;qs=n&amp;amp;cvid=349E7D38E6BE4845BE9C361D92FD2375&amp;amp;first=1&amp;amp;scenario=ImageBasicHover&quot;&gt;healthy skin&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_2_Production_files/FastAI2020/Lesson_2_Production/Fastbook_Chapter_2_Production/healthy_skin.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be careful about using images without inspecting them - even commercial search images can produce biased results.&lt;/li&gt;
  &lt;li&gt;See “&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3306618.3314244&quot;&gt;Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products&lt;/a&gt;” for more examples&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;from-data-to-dataloaders&quot;&gt;From Data to DataLoaders&lt;/h2&gt;

&lt;p&gt;Now that data has been captured and stored in segregated folders, we need a process to load the image data and feed them to model for training.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The components responsible for loading the data are the &lt;strong&gt;DataLoaders&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;DataLoaders&lt;/em&gt; are composed of multiple &lt;em&gt;*DataLoader **objects passed to it - normally the *train&lt;/em&gt; dataloader and the &lt;em&gt;valid&lt;/em&gt; dataloader&lt;/p&gt;

    &lt;p&gt;class DataLoaders(GetAttr):
      def &lt;strong&gt;init&lt;/strong&gt;(self, *loaders): self.loaders = loaders
      def &lt;strong&gt;getitem&lt;/strong&gt;(self, i): return self.loaders[i]
      train,valid = add_props(lambda i,self: self[i])&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;DataLoader specifications:
    &lt;ul&gt;
      &lt;li&gt;What kinds of data we are working with (Image, Category)&lt;/li&gt;
      &lt;li&gt;How to get a list of items&lt;/li&gt;
      &lt;li&gt;How to label these items&lt;/li&gt;
      &lt;li&gt;How to create the validation set&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DataLoaders have &lt;em&gt;factory methods&lt;/em&gt; for the most common combinations of these components
    &lt;ul&gt;
      &lt;li&gt;example: &lt;code class=&quot;highlighter-rouge&quot;&gt;ImageDataLoaders.from_name_func(&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MORE FLEXIBLE ALTERNATIVE is the &lt;strong&gt;DataBlock&lt;/strong&gt; API&lt;/p&gt;

    &lt;p&gt;bears = DataBlock(
      blocks=(ImageBlock, CategoryBlock), 
      get_items=get_image_files, 
      splitter=RandomSplitter(valid_pct=0.2, seed=42),
      get_y=parent_label,
      item_tfms=Resize(128))&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;blocks=(ImageBlock, CategoryBlock) -&lt;/code&gt; tuple to specify what types for dependent (category - type of bear) and independent(image) variables&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;get_items=get_image_files&lt;/code&gt; - underlying items are file paths (to each image)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;splitter=RandomSplitter(...&lt;/code&gt; - randomly split 20 percent of data for validation and the rest for training, with a seed=42 so that the split is repeatable)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h3&gt;

&lt;h2 id=&quot;training-your-model-to-clean-your-data&quot;&gt;Training your Model to Clean your Data&lt;/h2&gt;

&lt;h2 id=&quot;turning-your-model-into-an-online-application&quot;&gt;Turning your Model into an Online Application&lt;/h2&gt;

&lt;h3 id=&quot;using-the-model-for-inference&quot;&gt;Using the Model for Inference&lt;/h3&gt;

&lt;h3 id=&quot;creating-a-notebook-app-from-the-model&quot;&gt;Creating a Notebook App from the Model&lt;/h3&gt;

&lt;h3 id=&quot;turning-your-notebook-into-a-real-app&quot;&gt;Turning Your Notebook into a Real App&lt;/h3&gt;

&lt;h3 id=&quot;deploying-your-app&quot;&gt;Deploying your app&lt;/h3&gt;

&lt;h2 id=&quot;how-to-avoid-disaster&quot;&gt;How to Avoid Disaster&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_2_Production_files/FastAI2020/Lesson_2_Production/Fastbook_Chapter_2_Production/att_00061.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;unforeseen-consequences-and-feedback-loops&quot;&gt;Unforeseen Consequences and Feedback Loops&lt;/h3&gt;

&lt;h2 id=&quot;get-writing&quot;&gt;Get Writing&lt;/h2&gt;</content><author><name></name></author><summary type="html">These are my notes from my reading of the fastai book by Jeremy Howard and Sylvain Gugger</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://butchland.github.io/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-00-49-fastbook.png" /><media:content medium="image" url="https://butchland.github.io/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-00-49-fastbook.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fastbook Chapter 1 Introduction</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/22/fastbook-chapter-1-introduction.html" rel="alternate" type="text/html" title="Fastbook Chapter 1 Introduction" /><published>2020-06-22T00:00:00-05:00</published><updated>2020-06-22T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/22/fastbook-chapter-1-introduction</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/22/fastbook-chapter-1-introduction.html">&lt;p&gt;&lt;em&gt;These are my notes from my reading of the fastai book&lt;/em&gt;
&lt;em&gt;by Jeremy Howard and Sylvain Gugger&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-00-49-fastbook.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;your-deep-learning-journey&quot;&gt;Your Deep Learning Journey&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Deep Learning is for Everyone&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Deep Learning (DL)&lt;/strong&gt;  is a powerful tool that uses data to build powerful applications that would otherwise have been impossible to build using normal programming techniques.
    &lt;ul&gt;
      &lt;li&gt;Can be applied across many disciplines&lt;/li&gt;
      &lt;li&gt;Domain experts can find new applications for it&lt;/li&gt;
      &lt;li&gt;Need more people with different backgrounds to get involved and start using it&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.fast.ai/&quot;&gt;fast.ai&lt;/a&gt; was founded to spread DL into the hands of as many people as possible&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Accessibility:&lt;/strong&gt; DL is a technology that is accessible to normal folks. You don’t need:
    &lt;ul&gt;
      &lt;li&gt;lots of math&lt;/li&gt;
      &lt;li&gt;lots of data&lt;/li&gt;
      &lt;li&gt;lots of expensive computers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-06-41-what-you-dont-need-in-dl.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: high school math, some coding experience required preferably Python
    &lt;ul&gt;
      &lt;li&gt;but Python is easy to learn, lots of &lt;a href=&quot;https://www.learnpython.org/&quot;&gt;free online courses&lt;/a&gt; teach it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Why learn DL?&lt;/strong&gt;  - Areas/Fields/Disciplines where DL is now best in the world:
    &lt;ul&gt;
      &lt;li&gt;Natural Language Processing (NLP)&lt;/li&gt;
      &lt;li&gt;Computer Vision (CV)&lt;/li&gt;
      &lt;li&gt;Medicine&lt;/li&gt;
      &lt;li&gt;Biology&lt;/li&gt;
      &lt;li&gt;Image Generation&lt;/li&gt;
      &lt;li&gt;Recommendation Systems&lt;/li&gt;
      &lt;li&gt;Playing Games&lt;/li&gt;
      &lt;li&gt;Robotics&lt;/li&gt;
      &lt;li&gt;Others&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-08-41-dl-sota-areas.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;neural-networks-a-brief-history&quot;&gt;Neural Networks: A Brief History&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-10-08-neural-networks-rosenblatt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Warren McCulloch and Walter Pitts, 1943&lt;/strong&gt; - developed mathematical model of artificial neuron&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Frank Rosenblatt,&lt;/strong&gt; gave the artificial neural network (NN) the ability to learn, and built the &lt;em&gt;Mark 1 Perceptron&lt;/em&gt;, a machine capable of recognizing simple shapes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Marvin Minsky &amp;amp; Seymour Papert&lt;/strong&gt; wrote &lt;em&gt;Perceptrons&lt;/em&gt; (book)
    &lt;ul&gt;
      &lt;li&gt;asserted that NNs are limited
        &lt;ul&gt;
          &lt;li&gt;1 layer NNs can’t even compute XOR&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;but also asserted 2 layers can compute more complex functions
        &lt;ul&gt;
          &lt;li&gt;this was kind of disregarded, instead people focused only on the 1st assertion&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;as a result, research on NNs was almost non-existent in the next 2 decades
        &lt;ul&gt;
          &lt;li&gt;led to first AI winter&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In 1986, MIT published the 2 volume book &lt;strong&gt;Parallel Distributed Processing (PDP)&lt;/strong&gt;,  where it asserts that a NN-based system requires:
    &lt;ul&gt;
      &lt;li&gt;processing units&lt;/li&gt;
      &lt;li&gt;activation state&lt;/li&gt;
      &lt;li&gt;output function&lt;/li&gt;
      &lt;li&gt;pattern of connectivity&lt;/li&gt;
      &lt;li&gt;propagation rule&lt;/li&gt;
      &lt;li&gt;activation rule&lt;/li&gt;
      &lt;li&gt;learning rule&lt;/li&gt;
      &lt;li&gt;environment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-10-38-pdp-pipeline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;80’s and 90s -&lt;/strong&gt; 2 layer NN began to widely used for real practical projects. but were too big or to slow&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;90s&lt;/strong&gt; researchers showed that more layers needed to get practical good performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;last decade&lt;/strong&gt; - increases in data availability, improvements in hardware, algorithmic tweaks made deep neural networks practical and powerful.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;who-we-are&quot;&gt;Who We Are&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jeremy, Sylvain, Rachel
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.fast.ai/about/#founders&quot;&gt;About FastAI Team&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.fast.ai/images/jh-head.jpg&quot; alt=&quot;headpic&quot; title=&quot;Jeremy Howard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.fast.ai/images/thomas.JPG&quot; alt=&quot;headpic&quot; title=&quot;Rachel Thomas&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.fast.ai/images/sg-head.jpg&quot; alt=&quot;headpic&quot; title=&quot;Sylvain Gugger&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-learn-dl&quot;&gt;How to learn DL&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Based on work by David Perkins, “Making Learning Whole”
    &lt;ul&gt;
      &lt;li&gt;learn the whole game&lt;/li&gt;
      &lt;li&gt;learn through examples&lt;/li&gt;
      &lt;li&gt;simplify as much as possible&lt;/li&gt;
      &lt;li&gt;remove barriers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-16-38-play-whole-game.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Code and try to solve problems - theory can come later.&lt;/li&gt;
  &lt;li&gt;Much of deep learning is still artisanal and can only be learned by actual experience in building models and datasets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tenacity is key&lt;/strong&gt; - getting stuck is normal. rewind and read slowly if stuck, experiment and google.&lt;/li&gt;
  &lt;li&gt;Apply to personal projects so you can sustain interest&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projects-and-mindset&quot;&gt;Projects and Mindset&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Get good test cases and projects&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;focus on hobbies and passion&lt;/li&gt;
      &lt;li&gt;try something not to ambitious at first, so that you don’t get stuck.&lt;/li&gt;
      &lt;li&gt;set 4 or 5 little projects instead of 1 grand project.&lt;/li&gt;
      &lt;li&gt;once you’ve done some little projects, go for bigger project you can show off.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Character traits for success&lt;/strong&gt;: playfulness and curiosity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-software-stack-pytorch-fastai-jupyter&quot;&gt;The Software Stack: Pytorch, fastai, Jupyter&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-23-35-fastai-pytorch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pytorch&lt;/strong&gt; &lt;a href=&quot;https://pytorch.org&quot;&gt;provides&lt;/a&gt; low level foundation&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;fastai&lt;/strong&gt; &lt;a href=&quot;https://dev.fast.ai&quot;&gt;provides&lt;/a&gt; higher level abstractions and productivity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-25-57-fastai-paper.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Jupyter notebook&lt;/strong&gt; - &lt;a href=&quot;https://jupyter.org/&quot;&gt;provides&lt;/a&gt; the experimentation and documentation environment&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;first-model&quot;&gt;First Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Setup your GPU DL Server
    &lt;ul&gt;
      &lt;li&gt;use Cloud (e.g. &lt;a href=&quot;https://colab.research.google.com&quot;&gt;Colab&lt;/a&gt; or &lt;a href=&quot;https://gradient.paperspace.com/&quot;&gt;Paperspace&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Run your first notebook
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/fastai/course-v4&quot;&gt;Course notebooks site&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://book.fast.ai&quot;&gt;Book site&lt;/a&gt; and &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;book notebooks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;setup account (gmail for colab, email for paperspace)
        &lt;ul&gt;
          &lt;li&gt;for paperspace - see &lt;a href=&quot;https://course.fast.ai/start_gradient.html&quot;&gt;paperspace setup&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;for colab - see &lt;a href=&quot;https://course.fast.ai/start_colab.html&quot;&gt;colab setup&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;load notebooks
        &lt;ul&gt;
          &lt;li&gt;for colab
            &lt;ul&gt;
              &lt;li&gt;modify notebook for colab environment - see &lt;a href=&quot;https://forums.fast.ai/t/platform-colab-free-10-month-pro/65525&quot;&gt;forum post&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;setup google drive to clone course notebooks&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;view &lt;a href=&quot;https://github.com/fastai/course-v4/blob/master/nbs/app_jupyter.ipynb&quot;&gt;app_jupyter.ipynb&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;run &lt;a href=&quot;https://github.com/fastai/course-v4/blob/master/nbs/01_intro.ipynb&quot;&gt;first notebook&lt;/a&gt;: Cats and Dogs Image Classifier
        &lt;ul&gt;
          &lt;li&gt;duplicate and save notebook&lt;/li&gt;
          &lt;li&gt;get dataset, create dataloader, create learner, train model&lt;/li&gt;
          &lt;li&gt;do inference on model&lt;/li&gt;
          &lt;li&gt;build image uploader and run inference on uploader using trained model: &lt;em&gt;is this a cat?&lt;/em&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/cute-kitty.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-machine-learning-ml&quot;&gt;What is Machine Learning (ML)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Deep learning is a modern area in the general discipline of ML&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Normal Programs&lt;/strong&gt; - make computer do a task by giving it detailed instructions
    &lt;ul&gt;
      &lt;li&gt;inputs → program → outputs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/programs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ML is an ALTERNATIVE way to get computers to do a task by giving it examples&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Arthur Samuels formulation (IBM, 1949) - 1962 essay - &lt;em&gt;AI: A frontier of automation:&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;weights&lt;/strong&gt; are assigned
        &lt;ul&gt;
          &lt;li&gt;weights are variables - given a particular choice of values&lt;/li&gt;
          &lt;li&gt;inputs are values processed to produce results,&lt;/li&gt;
          &lt;li&gt;weights are other values that define how the program will operate.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;every weight assignment &lt;strong&gt;results&lt;/strong&gt; in some level of &lt;strong&gt;performance&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;automated means of &lt;strong&gt;measuring performance&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;mechanism for &lt;strong&gt;improving performance&lt;/strong&gt; by weight assignments&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Training Models&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;inputs + weights → model → outputs + labels → performance → (update) → weights&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/trainmodels2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;using modern terms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/trainmodels3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cat Dog Image Recognition Example
    &lt;ul&gt;
      &lt;li&gt;inputs  -&amp;gt; images of cats and dogs&lt;/li&gt;
      &lt;li&gt;parameters -&amp;gt; “weights”&lt;/li&gt;
      &lt;li&gt;outputs -&amp;gt; predict whether its a cat or dog&lt;/li&gt;
      &lt;li&gt;label -&amp;gt; actual value whether image is a cat or dog&lt;/li&gt;
      &lt;li&gt;loss - if output matches label -&amp;gt; loss is low, if output does not match label -&amp;gt; loss is high&lt;/li&gt;
      &lt;li&gt;after measuring loss, update the parameters using SGD&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Inference (after training)
    &lt;ul&gt;
      &lt;li&gt;inputs + weights (now part of model) → outputs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/trainmodels.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-neural-network-nn&quot;&gt;What is a Neural Network (NN)?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A function that can compute any set of outputs given a set of inputs&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Universal Approximation Theorem&lt;/strong&gt; - &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;shows&lt;/a&gt; that neural networks can solve any problem to any level of accuracy.&lt;/li&gt;
  &lt;li&gt;To find a way to update the “weights”  of a NN in order to improve its performance, we use &lt;strong&gt;Stochastic Gradient Descent (&lt;/strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;SGD&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-learning-jargon&quot;&gt;Deep Learning Jargon&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Architecture&lt;/em&gt; - functional form of the model&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Parameters&lt;/em&gt; - “weights” that form the model&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Predictions&lt;/em&gt; are calculated from the &lt;em&gt;independent variables&lt;/em&gt;, which is the &lt;em&gt;input data&lt;/em&gt; not including the &lt;em&gt;labels&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Outputs&lt;/em&gt; of the model given its &lt;em&gt;inputs&lt;/em&gt; (which are the &lt;em&gt;independent variables&lt;/em&gt;) are called &lt;em&gt;predictions *and are also considered the *dependent variables&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;The measure of &lt;em&gt;performance&lt;/em&gt; is called the &lt;em&gt;loss&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Loss&lt;/em&gt; is dependent on the &lt;em&gt;predictions&lt;/em&gt; plus the correct &lt;em&gt;labels&lt;/em&gt; aka &lt;em&gt;targets&lt;/em&gt; or &lt;em&gt;dependent variables&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;inputs + parameters → model architecture → predictions (outputs) + labels → loss → (update) parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/trainmodels3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations-inherent-in-ml&quot;&gt;Limitations inherent in ML&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A model cannot be created without data&lt;/li&gt;
  &lt;li&gt;A model can only learn to operate on patterns seen in the input data used to train it&lt;/li&gt;
  &lt;li&gt;The learning approach creates &lt;em&gt;predictions&lt;/em&gt; not recommended actions&lt;/li&gt;
  &lt;li&gt;It is not enough to have examples of input data, we also need the &lt;em&gt;labels&lt;/em&gt; for the input data&lt;/li&gt;
  &lt;li&gt;Need to think about how ML is applied since its predictions are sometimes used for recommendation systems that can predict what a user is most likely to do.&lt;/li&gt;
  &lt;li&gt;Also need to think about the &lt;em&gt;environment&lt;/em&gt; where ML is applied and how it interacts with the environment it is getting its data from.
    &lt;ul&gt;
      &lt;li&gt;example : recommender systems creating feedback loops
        &lt;ul&gt;
          &lt;li&gt;predictive policing model - amplifies existing racial bias in policing by predicting more crime in areas where POC are dominant and recommends more policing in those areas, leading to more arrests — arrests proxy for crime, so data (and models based on the data) will lead to a positive feedback loop.&lt;/li&gt;
          &lt;li&gt;youtube videos recommending more extremist views to increase engagement — since videos with extremist views tend to be more engaging, system recommends those videos, leading viewers to espouse more extremist views and encouraging them to engage more with those types of extremist videos.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-the-image-recognizer-works&quot;&gt;How the Image Recognizer works&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Model Training&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Code sample&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from fastai2.vision.all import *

path = untar_data(URLs.PETS)

def is_cat(item): return item.name[0].isupper()

dls = ImageDataLoaders.from_name_func(path, get_image_files(path),
          label_func=is_cat,item_tfms=Resize(224),valid_pct=0.2, 
          seed=42)
learn = cnn_learner(dls,resnet34, metrics=error_rate)
learn.fine_tune(1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;from fastai2.vision.all import * :&lt;/code&gt; import fastai library  - fastai2.vision.all package contains all the needed functions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;path = untar_data(URLs.PETS)&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; download data from a URL&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;def is_cat(item)...:&lt;/code&gt; define function to determine label based on filename
    &lt;ul&gt;
      &lt;li&gt;for the PETS dataset, if the name of the file is capitalized, its a cat, otherwise its a dog&lt;/li&gt;
      &lt;li&gt;the type of the output or label determines the type of ML problem:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;regression&lt;/strong&gt; - when output is continuous value&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;classification&lt;/strong&gt; - when output is a discrete set of values&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dls = ImageDataLoaders.from_name_func...&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; defines how to load data that will be fed to the model using a &lt;em&gt;dataloader&lt;/em&gt; object
    &lt;ul&gt;
      &lt;li&gt;what type of data is input (&lt;em&gt;Image&lt;/em&gt;)&lt;/li&gt;
      &lt;li&gt;what output is expected (&lt;code class=&quot;highlighter-rouge&quot;&gt;label_func=is_cat&lt;/code&gt; to determine target label)&lt;/li&gt;
      &lt;li&gt;what transforms need to be applied to input data.
        &lt;ul&gt;
          &lt;li&gt;item transforms - applied to each item (&lt;code class=&quot;highlighter-rouge&quot;&gt;Resize(224)&lt;/code&gt;)
            &lt;ul&gt;
              &lt;li&gt;224 is standard for historical reasons (pretrained model used 224x224 as image size)&lt;/li&gt;
              &lt;li&gt;can increase size for better detail and possibly better results, but more resource intensive, reducing size reduces detail, but can run much faster with less memory&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;batch transforms - applied to a batch of items (typically on GPU so its fast)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;split &lt;em&gt;validation&lt;/em&gt; from &lt;em&gt;training&lt;/em&gt; sets - by default 20% of input is randomly selected as validation with a set &lt;code class=&quot;highlighter-rouge&quot;&gt;seed=42&lt;/code&gt; (to fix the splitting)
        &lt;ul&gt;
          &lt;li&gt;&lt;em&gt;why split into training and validation sets?&lt;/em&gt;
            &lt;ul&gt;
              &lt;li&gt;NNs can “memorize” the training data, which will make model predict well with data used in training, but will not generalize to other data not used in training.&lt;/li&gt;
              &lt;li&gt;By setting aside data in a validation set, this data in the validation set is not used in training, but is used to monitor &lt;em&gt;overfitting&lt;/em&gt;.&lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; occurs when NN is optimizing on training data but performance on validation set is worsening.
                &lt;ul&gt;
                  &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; is one of the most important and challenging issue in ML&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;learn = cnn_learner(..&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; create cnn learner with architecture and metrics using dataloader
    &lt;ul&gt;
      &lt;li&gt;set &lt;em&gt;validation metrics&lt;/em&gt; to show model performance and monitor &lt;strong&gt;overfitting&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt; is different from &lt;strong&gt;loss&lt;/strong&gt; - metrics measure &lt;em&gt;quality&lt;/em&gt; of model predictions, loss is used for tracking how the &lt;em&gt;performance&lt;/em&gt; changes as model parameters are changed (used in training)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;set the &lt;em&gt;architecture&lt;/em&gt; - in our case &lt;code class=&quot;highlighter-rouge&quot;&gt;resnet34&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;different architectures available
        &lt;ul&gt;
          &lt;li&gt;example &lt;em&gt;resnet34&lt;/em&gt; comes with 34 layers, &lt;em&gt;resnet50&lt;/em&gt; with 50, &lt;em&gt;resnet152&lt;/em&gt; with 152 layers, etc
            &lt;ul&gt;
              &lt;li&gt;visualize the NN as consisting of layered cake - with the inputs (images) entering from the bottom and the predictions coming out from the top (aka the head)…&lt;/li&gt;
              &lt;li&gt;NN with 50 and 152 layers are bigger than one with 34 layers, resnet18 is the smallest in the resnet family.&lt;/li&gt;
              &lt;li&gt;smaller NNs are faster to train, larger NNs are more powerful in capturing nuances (in general)&lt;/li&gt;
              &lt;li&gt;to find the ideal size, you have to experiment - but resnet34 is usually a good compromise and default&lt;/li&gt;
              &lt;li&gt;there are also other families and variations within familities of architectures&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;pretrained flag - default to true&lt;/strong&gt; - use pretrained weights - where a model trained in a different related task is reused for another task, reducing the amount of training time and data needed to achieve good performance.
    &lt;ul&gt;
      &lt;li&gt;pretrained will remove the last (topmost) layer and replace with 1 or more randomised layers specific for your task.&lt;/li&gt;
      &lt;li&gt;using a pretrained model is &lt;strong&gt;the most important method&lt;/strong&gt; to allow us to train accurate models, more quickly with less data and less time and money.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transfer learning&lt;/strong&gt; - using a pretrained model for a task different from what it was trained for. 
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Most important technique&lt;/strong&gt; to reduce resources (data, compute, time) needed to train models to acceptable or even state of the art (SOTA) performance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;learn.fine_tune(1)&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; fit (i.e. train)  the model to your task.
    &lt;ul&gt;
      &lt;li&gt;1 is the number of epochs - the number of times your model looks at each sample of input data.&lt;/li&gt;
      &lt;li&gt;there is also a &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; method that does fit the model, but &lt;code class=&quot;highlighter-rouge&quot;&gt;fine_tune&lt;/code&gt; does additional “tricks” to adapt a pretrained model for a new dataset, a process known as &lt;em&gt;fine tuning&lt;/em&gt;.
        &lt;ul&gt;
          &lt;li&gt;fine tune - 1. use one epoch (aka freeze_epochs parameter defaulted to 1) - to train just the head, with the rest of the model frozen&lt;/li&gt;
          &lt;li&gt;unfreeze the lower layers, and train using the &lt;em&gt;discriminative learning rates&lt;/em&gt;, where the lower layers (which need less adjustments since it has already been pretrained) are trained with lower learning rate, the &lt;em&gt;head&lt;/em&gt; (which starts with random values) is trained with higher rates
            &lt;ul&gt;
              &lt;li&gt;the &lt;em&gt;learning rate&lt;/em&gt; is a “&lt;em&gt;hyperparameter&lt;/em&gt;” - a knob that is used to tweak the training of the model in order to make the model achieve a better level of performance faster.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-our-image-recognizer-learned&quot;&gt;What our Image Recognizer Learned&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;the model might be performing well, but its still a “black box” as to what its really doing (this is a known problem in ML).&lt;/li&gt;
  &lt;li&gt;there are techniques to inspect DL models and get insights, but it can be challenging to understand, especially when they encounter data that is very different from the one used in training the model.&lt;/li&gt;
  &lt;li&gt;Zeiler and Fergus, 2013 - &lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;Visualizing and Understanding CNNs&lt;/a&gt; - visualization of NN weights learned in each layer.
    &lt;ul&gt;
      &lt;li&gt;lower layers - simple shapes&lt;/li&gt;
      &lt;li&gt;higher layers - more complex shapes combined from lower layers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/layer1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/layer2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/chapter2_layer3.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/chapter2_layer4and5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model (Alexnet) studied had only 5 layers so deeper networks even can recognize even more complex features.
The upper layers (ie. the head) are specialized for the task specific to your model (distinguishing cats from dogs) and so require higher learning rates compared to lower layers which share more or less the same features as the pretrained model.&lt;/p&gt;

&lt;h2 id=&quot;extending-image-recognizers-to-handle-non-image-tasks&quot;&gt;Extending Image Recognizers to handle Non-Image Tasks&lt;/h2&gt;

&lt;p&gt;One way to extend image recognizers is by converting data into images which can then reuse pretrained image models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sound - converted to spectograms - see this &lt;a href=&quot;https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52&quot;&gt;article&lt;/a&gt; using fastai for audio classification&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/att_00012.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mouse movements - Splunk converted recorded mouse movements into images which were then used to train a bot detector - &lt;a href=&quot;https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html&quot;&gt;see this article&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/att_00014.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;General tip:&lt;/strong&gt; if your data can be converted to image data and the images can be classified by humans looking at the images, chances are image recognizers using transfer learning can be trained to classify them as well.&lt;/p&gt;

&lt;h2 id=&quot;more-jargon&quot;&gt;More Jargon&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Label&lt;/li&gt;
  &lt;li&gt;Architecture&lt;/li&gt;
  &lt;li&gt;Model&lt;/li&gt;
  &lt;li&gt;Parameters&lt;/li&gt;
  &lt;li&gt;Fit&lt;/li&gt;
  &lt;li&gt;Train&lt;/li&gt;
  &lt;li&gt;Pretrained Model&lt;/li&gt;
  &lt;li&gt;Fine tuning&lt;/li&gt;
  &lt;li&gt;Epoch&lt;/li&gt;
  &lt;li&gt;Loss&lt;/li&gt;
  &lt;li&gt;Metric&lt;/li&gt;
  &lt;li&gt;Generalization&lt;/li&gt;
  &lt;li&gt;Overfitting&lt;/li&gt;
  &lt;li&gt;Training Set&lt;/li&gt;
  &lt;li&gt;Validation Sets&lt;/li&gt;
  &lt;li&gt;Convolutional NNs&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-applications-of-dl&quot;&gt;Other applications of DL&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Image Segmentation&lt;/strong&gt; - label each pixel with object it belongs to
    &lt;ul&gt;
      &lt;li&gt;CamVid example using subset of data from the paper &lt;a href=&quot;http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/&quot;&gt;Semantic Object Classes in Video: A High-Definition Ground Truth Database&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/dlcp_01in03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sentiment Classification&lt;/strong&gt; (Natural Language Processing or NLP) - tells you if movie review is positive or negative
    &lt;ul&gt;
      &lt;li&gt;Movie Review Sentiment Classification using IMDB data from the paper &lt;a href=&quot;https://ai.stanford.edu/~amaas/data/sentiment/&quot;&gt;Learning Word Vectors for Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tabular Data&lt;/strong&gt; - predicts income based on multiple factors such as age, educational attainment etc.
    &lt;ul&gt;
      &lt;li&gt;Income prediction using &lt;a href=&quot;https://archive.ics.uci.edu/ml/datasets/adult&quot;&gt;Adult dataset&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recommendation Systems&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Movie Recommendation using &lt;a href=&quot;https://doi.org/10.1145/2827872&quot;&gt;movie lens dataset&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;validation-sets-and-test-sets&quot;&gt;Validation Sets and Test Sets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;train set&lt;/strong&gt; - data seen by model during training&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;validation set&lt;/strong&gt; - used for evaluating model - test model to generalize and tweak it appropriately (hyper parameter tuning)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;test set&lt;/strong&gt; - data reserved for predicting performance in the real world (after tweaking)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TIP&lt;/strong&gt; for creating validation and test sets - simulate data that will be encountered in production - random subsets may not always be best way to select validation - e.g. time series data - should use continuous data in the future w.r.t. to data used or images not in training data (assuming they occur multiple times)&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">These are my notes from my reading of the fastai book by Jeremy Howard and Sylvain Gugger</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://butchland.github.io/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-00-49-fastbook.png" /><media:content medium="image" url="https://butchland.github.io/butchland-machine-learning-notes/assets/images/Fastbook_Chapter_1_Introduction_files/FastAI2020/Lesson_1_Introduction/Fastbook_Chapter_1_Introduction/00-00-49-fastbook.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fastbook Chapter 4 Questionnaire Answers</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/03/fast-ai-chapter-4-mnist-questionnaire-answers.html" rel="alternate" type="text/html" title="Fastbook Chapter 4 Questionnaire Answers" /><published>2020-06-03T00:00:00-05:00</published><updated>2020-06-03T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/03/fast-ai-chapter-4-mnist-questionnaire-answers</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/03/fast-ai-chapter-4-mnist-questionnaire-answers.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-03-fast-ai-chapter-4-mnist-questionnaire-answers.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Questionnaire&quot;&gt;Questionnaire&lt;a class=&quot;anchor-link&quot; href=&quot;#Questionnaire&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;How is a greyscale image represented on a computer? How about a color image?&lt;/li&gt;
&lt;li&gt;How are the files and folders in the &lt;code&gt;MNIST_SAMPLE&lt;/code&gt; dataset structured? Why?&lt;/li&gt;
&lt;li&gt;Explain how the &quot;pixel similarity&quot; approach to classifying digits works.&lt;/li&gt;
&lt;li&gt;What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.&lt;/li&gt;
&lt;li&gt;What is a &quot;rank 3 tensor&quot;?&lt;/li&gt;
&lt;li&gt;What is the difference between tensor rank and shape? How do you get the rank from the shape?&lt;/li&gt;
&lt;li&gt;What are RMSE and L1 norm?&lt;/li&gt;
&lt;li&gt;How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?&lt;/li&gt;
&lt;li&gt;Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.&lt;/li&gt;
&lt;li&gt;What is broadcasting?&lt;/li&gt;
&lt;li&gt;Are metrics generally calculated using the training set, or the validation set? Why?&lt;/li&gt;
&lt;li&gt;What is SGD?&lt;/li&gt;
&lt;li&gt;Why does SGD use mini batches?&lt;/li&gt;
&lt;li&gt;What are the 7 steps in SGD for machine learning?&lt;/li&gt;
&lt;li&gt;How do we initialize the weights in a model?&lt;/li&gt;
&lt;li&gt;What is &quot;loss&quot;?&lt;/li&gt;
&lt;li&gt;Why can't we always use a high learning rate?&lt;/li&gt;
&lt;li&gt;What is a &quot;gradient&quot;?&lt;/li&gt;
&lt;li&gt;Do you need to know how to calculate gradients yourself?&lt;/li&gt;
&lt;li&gt;Why can't we use accuracy as a loss function?&lt;/li&gt;
&lt;li&gt;Draw the sigmoid function. What is special about its shape?&lt;/li&gt;
&lt;li&gt;What is the difference between loss and metric?&lt;/li&gt;
&lt;li&gt;What is the function to calculate new weights using a learning rate?&lt;/li&gt;
&lt;li&gt;What does the &lt;code&gt;DataLoader&lt;/code&gt; class do?&lt;/li&gt;
&lt;li&gt;Write pseudo-code showing the basic steps taken each epoch for SGD.&lt;/li&gt;
&lt;li&gt;Create a function which, if passed two arguments &lt;code&gt;[1,2,3,4]&lt;/code&gt; and &lt;code&gt;'abcd'&lt;/code&gt;, returns &lt;code&gt;[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]&lt;/code&gt;. What is special about that output data structure?&lt;/li&gt;
&lt;li&gt;What does &lt;code&gt;view&lt;/code&gt; do in PyTorch?&lt;/li&gt;
&lt;li&gt;What are the &quot;bias&quot; parameters in a neural network? Why do we need them?&lt;/li&gt;
&lt;li&gt;What does the &lt;code&gt;@&lt;/code&gt; operator do in python?&lt;/li&gt;
&lt;li&gt;What does the &lt;code&gt;backward&lt;/code&gt; method do?&lt;/li&gt;
&lt;li&gt;Why do we have to zero the gradients?&lt;/li&gt;
&lt;li&gt;What information do we have to pass to &lt;code&gt;Learner&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;Show python or pseudo-code for the basic steps of a training loop.&lt;/li&gt;
&lt;li&gt;What is &quot;ReLU&quot;? Draw a plot of it for values from &lt;code&gt;-2&lt;/code&gt; to &lt;code&gt;+2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;What is an &quot;activation function&quot;?&lt;/li&gt;
&lt;li&gt;What's the difference between &lt;code&gt;F.relu&lt;/code&gt; and &lt;code&gt;nn.ReLU&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Fastbook Chapter 3 Questionnaire Answers</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/03/fast-ai-chapter-3-ethics-questionnaire-answers.html" rel="alternate" type="text/html" title="Fastbook Chapter 3 Questionnaire Answers" /><published>2020-06-03T00:00:00-05:00</published><updated>2020-06-03T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/03/fast-ai-chapter-3-ethics-questionnaire-answers</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/03/fast-ai-chapter-3-ethics-questionnaire-answers.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-03-fast-ai-chapter-3-ethics-questionnaire-answers.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Questionnaire&quot;&gt;Questionnaire&lt;a class=&quot;anchor-link&quot; href=&quot;#Questionnaire&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;See &lt;a href=&quot;https://github.com/fastai/fastbook/blob/master/clean/03_ethics.ipynb&quot;&gt;Lesson 3&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Does ethics provide a list of &quot;right answers&quot;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;no list of right answers, each must evaluate their own perspectives and contexts of the ethical issues&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How can working with people of different backgrounds help when considering ethical questions?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;provide diverse perspectives and contexts for more rounded consideration of pros and cons of actions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;What was the role of the first person jailed in the Volkswagen diesel scandal?&lt;/li&gt;
&lt;li&gt;What was the problem with a database of suspected gang members maintained by California law enforcement officials?&lt;/li&gt;
&lt;li&gt;Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?&lt;/li&gt;
&lt;li&gt;What are the problems with the centrality of metrics?&lt;/li&gt;
&lt;li&gt;Why did Meetup.com not include gender in its recommendation system for tech meetups?&lt;/li&gt;
&lt;li&gt;What are the six types of bias in machine learning, according to Suresh and Guttag?&lt;/li&gt;
&lt;li&gt;Give two examples of historical race bias in the US.&lt;/li&gt;
&lt;li&gt;Where are most images in ImageNet from?&lt;/li&gt;
&lt;li&gt;In the paper &lt;a href=&quot;https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf&quot;&gt;&quot;Does Machine Learning Automate Moral Hazard and Error&quot;&lt;/a&gt; why is sinusitis found to be predictive of a stroke?&lt;/li&gt;
&lt;li&gt;What is representation bias?&lt;/li&gt;
&lt;li&gt;How are machines and people different, in terms of their use for making decisions?&lt;/li&gt;
&lt;li&gt;Is disinformation the same as &quot;fake news&quot;?&lt;/li&gt;
&lt;li&gt;Why is disinformation through auto-generated text a particularly significant issue?&lt;/li&gt;
&lt;li&gt;What are the five ethical lenses described by the Markkula Center?&lt;/li&gt;
&lt;li&gt;Where is policy an appropriate tool for addressing data ethics issues?&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Fastbook Chapter 2 Questionnaire Answers</title><link href="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/02/fast-ai-chapter-2-prod-questionnaire-answers.html" rel="alternate" type="text/html" title="Fastbook Chapter 2 Questionnaire Answers" /><published>2020-06-02T00:00:00-05:00</published><updated>2020-06-02T00:00:00-05:00</updated><id>https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/02/fast-ai-chapter-2-prod-questionnaire-answers</id><content type="html" xml:base="https://butchland.github.io/butchland-machine-learning-notes/fastai/2020/06/02/fast-ai-chapter-2-prod-questionnaire-answers.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-02-fast-ai-chapter-2-prod-questionnaire-answers.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Questionnaire&quot;&gt;Questionnaire&lt;a class=&quot;anchor-link&quot; href=&quot;#Questionnaire&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;See &lt;a href=&quot;https://github.com/fastai/fastbook/blob/master/clean/02_production.ipynb&quot;&gt;Lesson 2&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;when data used to train the model is very different from data used in production for inferencing.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Where do text models currently have a major deficiency?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;in generating correct responses to queries.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are possible negative societal implications of text generation models?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;automating disinformation campaigns using DL text models to generate simulated human responses.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;allow humans to oversee predictions made by ML models&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What kind of tabular data is deep learning particularly good at?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data with high cardinality categorical columns&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What's a key downside of directly using a deep learning model for recommendation systems?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;models that predict what items users would already buy or watch even if no recommendation was made instead of recommending items that a user might buy/watch but if not for the recommendation.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are the steps of the Drivetrain Approach?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Determine objectives&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Determine levers (inputs that can be controlled)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Determine data that can be collected to control the levers&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build the models&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do the steps of the Drivetrain Approach map to a recommendation system?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Objective:&lt;/strong&gt; create recommendations for items that users might like but otherwise would not be have if not for the recommendation and trigger new sales&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Levers:&lt;/strong&gt; ranking of the recommendation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; randomized experiments on a wide range of recommendations to a wide variety of customers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; &lt;ul&gt;
&lt;li&gt;2 Models for purchase probabilities conditional on seeing or not seeing a recommendation&lt;ol&gt;
&lt;li&gt;model - for purchases given a recommendation&lt;/li&gt;
&lt;li&gt;model - for purchases not given a recommendation&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;utility function for the difference in probabilities &lt;ul&gt;
&lt;li&gt;low &lt;ul&gt;
&lt;li&gt;when model recommends items which a customer is familiar with and rejected (both components low)&lt;/li&gt;
&lt;li&gt;when the model recommends items which a customer would have bought even with the recommendation (both high)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;high &lt;ul&gt;
&lt;li&gt;when the model recommends an item that a customer would buy if given a recommendation (large value) but low when the model does not recommend that item&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create an image recognition model using data you curate, and deploy it on the web.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is &lt;code&gt;DataLoaders&lt;/code&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;an object that specifies the way to feed data to a model&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What four things do we need to tell fastai to create &lt;code&gt;DataLoaders&lt;/code&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;what kinds of data we are working with&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;how to get the items&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;how to label items&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;how to split the validation set from the training set&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the &lt;code&gt;splitter&lt;/code&gt; parameter to &lt;code&gt;DataBlock&lt;/code&gt; do?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;how to split the validation set from the training set&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do we ensure a random split always gives the same validation set?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;by setting the random seed to a specific number ensures that the same pseudo random sequence is used each time to split the validation set from the training set&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What letters are often used to signify the independent and dependent variables?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;x for dependent, y for independent&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;crop - used by default - use full height or width and crop to make it square&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pad - add zeros to smaller dimension to make it square&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;squish - scale bigger dimension to fit into square&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;squish is used when it is important to include all the image data but the model can tolerate the distortion, crop is useful when partial crops of the image can still be useful in predicting the label, pad is useful when no distortion can be allowed but at the expense of having to pad the images which might decrease the model's performance.&lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is data augmentation? Why is it needed?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;alter the input slightly each time data goes through an epoch&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;improves model by reducing overfitting (not seeing the same data for each epoch due to distortions added by data augmentation)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the difference between &lt;code&gt;item_tfms&lt;/code&gt; and &lt;code&gt;batch_tfms&lt;/code&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;item_tfms - transforms applied once on each sample while reading the data.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;batch_tfms - applied to entire batch - usually using GPU so its fast.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is a confusion matrix?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a table plot that shows how many the model got right and wrong for each label category&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does &lt;a href=&quot;/butchland-machine-learning-notes/images/copied_from_nb/export&quot;&gt;&lt;code&gt;export&lt;/code&gt;&lt;/a&gt; save?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;an inference model - a model that can be used for making predictions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is it called when we use a model for getting predictions, instead of training?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;inferencing&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are IPython widgets?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;components that can add interactive functionality to a jupyter notebook&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When might you want to use CPU for deployment? When might GPU be better?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU is easier to deploy, when no training is needed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPU is faster when making lots of parallel inferences to run through the model&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;slower response due to roundtrip time, more centralized resources required&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are three examples of problems that could occur when rolling out a bear warning system in practice?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data used in training is not the same data used in production&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;video not images&lt;/li&gt;
&lt;li&gt;night time &lt;/li&gt;
&lt;li&gt;video camera feed vs internet pictures &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;slow response time&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is &quot;out-of-domain data&quot;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;data used in production is not part of training data&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is &quot;domain shift&quot;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;over time, data encountered in production is not same as used in training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are the three steps in the deployment process?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;manual oversight&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;limited scope deployment&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;gradual expansion&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Further-Research&quot;&gt;Further Research&lt;a class=&quot;anchor-link&quot; href=&quot;#Further-Research&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;Consider how the Drivetrain Approach maps to a project or problem you're interested in.&lt;/li&gt;
&lt;li&gt;When might it be best to avoid certain types of data augmentation?&lt;/li&gt;
&lt;li&gt;For a project you're interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot;&lt;/li&gt;
&lt;li&gt;Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you're interested in.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>